[
  {
    "objectID": "modelling_methods.html#commuting-flows",
    "href": "modelling_methods.html#commuting-flows",
    "title": "Modelling methods",
    "section": "Commuting flows",
    "text": "Commuting flows\nIn order to distribute each individual of the population to a unique physical workplace, we first created a population of all individual workplaces in England, based on a combination of the Nomis UK Business Counts 2020 dataset and the Nomis Business register and Employment Survey 2015 (see Data sources). The first dataset gives the number of individual workplace counts per industry, using the SIC 2007 industry classification, with imprecise size (i.e. number of employees) bands at MSOA level. The second dataset gives the total number of jobs available at LSOA level per SIC 2007 industry category. We found that the distribution of workplace sizes follows closely a simple 1/x distribution, allowing us to draw for each workplace a size within their band, with sum constraints given by the total number of jobs available, according to the second dataset.\nThe workplace ‘population’ and individual population are then levelled for each SIC 2007 category by removing the exceeding part of whichever dataset lists more items. This takes into account that people and business companies are likely to over-report their working availability (e.g. part time and seasonal contracts are not counted differently than full time contracts, jobseekers or people on maternity leave might report the SIC of their last job). This process can be controlled by a threshold in the parameter file that defines the maximal total proportion of workers or jobs that can be removed. If the two datasets cannot be levelled accordingly, the categories are dropped and the datasets are levelled globally. Tests in the West Yorkshire area have shown than when the level 1 SIC, containing 21 unique categories, is used, 90% of the volume of commuting flows were recovered compared to the Nomis commuting OD matrices at MSOA level.\nThe employees for each workplace are drawn according to the ‘universal law of visitation’, see\n\nSchläpfer M et al. The universal visitation law of human mobility. Nature 593, 522–527 (2021). (DOI)\n\nThis framework predicts that visitors to any destination follow a simple\n\nρ(r,f)= K / (rf)2\n\ndistribution, where ρ(r,f) is the density of visitors coming from a distance r with frequency f and K is a balancing constant depending on the specific area. In the context of commuting, it can be assumed that f = 1. Additionally, we only need to weigh potential employees against each other, which removes the necessity to compute explicitly K. In the West Yorkshire test, we found a Pearson coefficient of 0.7 between the predicted flows when aggregated at MSOA level and the OD matrix at MSOA level available from Nomis."
  },
  {
    "objectID": "developer_guide.html#updating-the-docs",
    "href": "developer_guide.html#updating-the-docs",
    "title": "Developer guide",
    "section": "Updating the docs",
    "text": "Updating the docs\nThe site is built with Quarto. You can iterate on it locally: cd docs; quarto preview"
  },
  {
    "objectID": "developer_guide.html#code-hygiene",
    "href": "developer_guide.html#code-hygiene",
    "title": "Developer guide",
    "section": "Code hygiene",
    "text": "Code hygiene\nWe use automated tools to format the code.\ncargo fmt\n\n# Format Markdown docs\nprettier --write *.md docs/*.md\nInstall prettier for Markdown."
  },
  {
    "objectID": "developer_guide.html#some-tips-for-working-with-rust",
    "href": "developer_guide.html#some-tips-for-working-with-rust",
    "title": "Developer guide",
    "section": "Some tips for working with Rust",
    "text": "Some tips for working with Rust\nThere are two equivalent ways to rebuild and then run the code. First:\ncargo run --release -- devon\nThe -- separates arguments to cargo, the Rust build tool, and arguments to the program itself. The second way:\ncargo build --release\n./target/release/aspics devon\nYou can build the code in two ways – debug and release. There’s a simple tradeoff – debug mode is fast to build, but slow to run. Release mode is slow to build, but fast to run. For the ASPICS codebase, since the input data is so large and the codebase so small, I’d recommend always using --release. If you want to use debug mode, just omit the flag.\nIf you’re working on the Rust code outside of an IDE like VSCode, then you can check if the code compiles much faster by doing cargo check."
  },
  {
    "objectID": "outputs.html#versioning",
    "href": "outputs.html#versioning",
    "title": "Outputs",
    "section": "Versioning",
    "text": "Versioning\nOver time, we may add more data to SPC or change the schema. Protocol buffers are designed to let combinations of new/old code and data files work together, but we don’t intend to use this feature. We may make breaking changes, like deleting fields. We’ll release a new version of the schema and output data every time and document it here. You should depend on a specific version of the data output in your code, so new releases don’t affect you until you decide to update.\n\nv1: released 25/04/2022, schema"
  },
  {
    "objectID": "data_sources.html#utility-data",
    "href": "data_sources.html#utility-data",
    "title": "Data sources",
    "section": "Utility data",
    "text": "Utility data\nlookUp.csv\nThe look-up table links different geographies together. It is used internally by the model, but can also help the user define their own study area. MSOA11CD, MSOA11NM, LAD20CD, LAD20NM, ITL321CD, ITL321NM, ITL221CD, ITL221NM, ITL121CD, ITL121NM are all standard denominations fully compatible with ONS fields of the same name. They are based on ONS lookups. See ONS documentation for more details. CTY20NM and CCTY20NM are custom denominations for the counties of England (used to sort the county level population data) and the ceremonial counties of England respectively. Their spelling may vary in different data sources and the field CTY20NM is not compatible with the ONS field of the same name (which excludes all counties that are also unitary authorities). GoogleMob and OSM are different spellings for the counties of England used by Google and OSM for their data releases."
  },
  {
    "objectID": "data_sources.html#county-level-data",
    "href": "data_sources.html#county-level-data",
    "title": "Data sources",
    "section": "County level data",
    "text": "County level data\nContains 47 files, each representing the population in 2020 of one of the counties of England mentioned above, and named\ntus_hse_<county_name>.gz\nThis data is based on the 2011 UK census, the Time Use Survey 2014-15 and the Health Survey for England 2017. The SPENSER (Synthetic Population Estimation and Scenario Projection) microsimulation model (reference) distributes a synthetic population based on the census at MSOA scale and projects it to 2020 according to estimates from the Office for National Statistics (ONS). This information was enriched with some of the content of the other two datasets through propensity score matching (PSM) by Prof. Karyn Morrissey (Technical University of Denmark). The rest of the datasets can be added a posteriori from the identifiers provided.\nThe fields currently contained are: - idp: a unique individual identifier within the present data - MSOA11CD: MSOA code where the individual lives - hid: household identifier, includes communal establishments - pid: identifier linking to the 2011 Census - pid_tus: identifier linking to the Time Use Survey 2015 - pid_hse: identifier linking to the Health Survey for England 2017 - sex: 0 female; 1 male - age: in years - origin: 1 White; 2 Black; 3 Asian; 4 Mixed; 5 Other - nssec5: National Statistics Socio-economic classification: - 1: Higher managerial, administrative and professional occupations - 2: Intermediate occupations - 3: Small employers and own account workers - 4: Lower supervisory and technical occupations - 5: Semi-routine and routine occupations - 0: Never worked and long-term unemployed - soc2010: Previous version of the Standard Occupational Classification - sic1d07: Standard Industrial Classification of Economic Activities 2007, 1st layer (number corresponding to the letter in alphabetical order) - sic2d07: Standard Industrial Classification of Economic Activities 2007, 2nd layer - Proportion of 24h spent doing different daily activities: - punknown + pwork + pschool + pshop + pservices + pleisure + pescort + ptransport = pnothome - phome + pworkhome = phometot - pnothome + phometot = 1 - cvd: has a cardio-vascular disease (0 or 1) - diabetes: has diabetes (0 or 1) - bloodpressure: has high blood pressure (0 or 1) - BMIvg6: Body Mass Index: - Not applicable - Underweight: less than 18.5 - Normal: 18.5 to less than 25 - Overweight: 25 to less than 30 - Obese I: 30 to less than 35 - Obese II: 35 to less than 40 - Obese III: 40 or more - lng: longitude of the MSOA11CD centroid - lat: latitude of the MSOA11CD centroid\nSome other fields were kept from specific projects but are not from official sources and should be used."
  },
  {
    "objectID": "data_sources.html#national-data",
    "href": "data_sources.html#national-data",
    "title": "Data sources",
    "section": "National data",
    "text": "National data\nbusinessRegistry.csv\nContains a breakdown of all business units (i.e. a single workplace) in England at LSOA scale (smaller than MSOA), estimated by the project contributors from two nomis datasets: UK Business Counts - local units by industry and employment size band 2020 and Business Register and Employment Survey 2015. Each item contains the size of the unit and its main sic1d07 code in reference to standard Industrial Classification of Economic Activities 2007 (number corresponding to the letter in alphabetical order). It is used to compute commuting flows.\nMSOAS_shp.tar.gz\nIs a simple shapefile taken from ONS boundaries.\nQUANT_RAMP.tar.gz\nSee: Milton R, Batty M, Dennett A, dedicated RAMP Spatial Interaction Model GitHub repository. It is used to compute the flows towards schools and retail.\ntimeAtHomeIncreaseCTY.csv\nThis file is a subset from Google COVID-19 Community Mobility Reports, cropped to England. It describes the daily reduction in mobility, averaged at county level, due to lockdown and other COVID-19 restrictions between the 15th of February 2020 and 15th of April 2022. Missing values have been replaced by the national average. These values can be used directly to reduce pnothome and increase phometot (and their sub-categories) to simulate more accurately the period."
  },
  {
    "objectID": "getting_started.html#python",
    "href": "getting_started.html#python",
    "title": "Getting started",
    "section": "Python",
    "text": "Python\nTo work with SPC protobufs in Python, you need two dependencies setup:\n\nThe protobuf library\n\nYou can install system-wide with pip install protobuf\nOr add as a dependency to a conda, poetry, etc environment\n\nThe generated Python library, synthpop_pb2.py\n\nYou can download a copy of this file into your codebase, then import synthpop_pb2\nYou can also generate the file yourself, following the docs: protoc --python_out=python/ synthpop.proto\n\n\n\nExample: converting to JSON\nTo interactively explore the data, viewing JSON is much easier. It shows the same structure as the protobuf, but in a human-readable text format. The example below uses a small Python script:\n# Download a file\nwget https://ramp0storage.blob.core.windows.net/spc-output/v1/rutland.pb.gz\n# Uncompress\ngunzip rutland.pb.gz\n# Convert the .pb to JSON\npython3 python/protobuf_to_json.py data/output/rutland.pb > rutland.json\n# View the output\nless rutland.json\n\n\nExample: convert to numpy arrays\nThe ASPICS project simulates the spread of COVID through a population. The code uses numpy, and this script converts the protobuf to a bunch of different numpy arrays.\nNote the code doesn’t keep using classes from the generated Python code for protobufs. The protobuf is a format optimized for reading and writing; you shouldn’t use it in your model if there’s a more appropriate tool you’re familiar with, like data frames."
  },
  {
    "objectID": "getting_started.html#example-draw-all-venues",
    "href": "getting_started.html#example-draw-all-venues",
    "title": "Getting started",
    "section": "Example: draw all venues",
    "text": "Example: draw all venues\nThis script reads a protobuf file, then draws a dot for every venue, color-coded by activity."
  },
  {
    "objectID": "getting_started.html#understand-the-schema",
    "href": "getting_started.html#understand-the-schema",
    "title": "Getting started",
    "section": "Understand the schema",
    "text": "Understand the schema\nHere are some helpful tips for understanding the schema.\nEach .pb file contains exactly one Population message. In contrast to datasets consisting of multiple .csv files, just a single file contains everything. Some of the fields in Population are lists (of people and households) or maps (of venues keyed by activity, or of MSOAs). Unlike a flat .csv table, there may be more lists embedded later. Each Household has a list of members, for example.\nThe different objects refer to each other, forming a graph structure. The protobuf uses uint64 IDs to index into other lists. For example, if some household has members = [3, 10], then those two people can be found at population.people[3] and population.people[10]. Each of them will have the same household ID, pointing back to something in the population.households list.\n\nFlows\nSPC models daily travel behavior of people as “flows.” Flows are broken down by by an activity – shopping/retail, attending primary or secondary school, working, or staying at home. For each activity type, a person has a list of venues where they may do that activity, weighted by a probability of going to that particular venue.\nNote that flows_per_activity is stored in InfoPerMSOA, not Person. The flows for retail and school are only known at the MSOA level, not individually. So given a particular Person object, you first look up their household’s MSOA – msoa = population.households[ person.household ].msoa and then look up flows for that MSOA – population.info_per_msoa[msoa].flows_per_activity.\nEach person has exactly 1 flow for home – it’s just person.household with probability 1. A person has 0 or 1 flows to work, based on the value of person.workplace.\nThis doesn’t mean that all people in the same MSOA share the same travel behavior. Each person has their own activity_durations field, based on time-use survey data. Even if two people share the same set of places where they may go shopping, one person may spend much more time on that activity than another.\nSee the ASPICS conversion script for all of this in action – it has a function to collapse a person’s flows down into a single weighted list.\nHow do you interpret the probabilities/weights for flows? If your model needs people to visit specific places each day, you could randomly sample a venue from the flows, weighting them appropriately. For retail, you may want to repeat this sampling every day of the simulation, so they visit different venues. For primary and secondary school, it may be more appropriate to sample once and store that for the simulation – a student probably doesn’t switch schools daily.\nAlternatively, you can follow what ASPICS does. Every day, each person logically visits all possible venues, but their interaction there (possibly receiving or transmitting COVID) is weighted by the probability of each venue."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Synthetic Population Catalyst",
    "section": "",
    "text": "SPC Schema\n\n\nThe Synthetic Population Catalyst (SPC) makes it easier for researchers to work with synthetic population data in England. It combines a variety of data sources and outputs a single file in protocol buffer format, describing the population in a given study area. The tool also provides methods to export the outcome in diferent formats often use for researchers like CSV or JSON.\nThe input of the SPC tool is a list of the Middle Layer Super Output Area (MSOAs) where you want to create a spatially enriched sythetic population to feed other dynamic models. SPC includes a script to assist you with the proper list of the MSOAs by defining a Local Authority District area in England. Get started to download SPC data or run the tool in different MSOAs."
  },
  {
    "objectID": "code_walkthrough.html#generally-useful-techniques",
    "href": "code_walkthrough.html#generally-useful-techniques",
    "title": "Code walkthrough",
    "section": "Generally useful techniques",
    "text": "Generally useful techniques\nThe code-base makes use of some techniques that may be generally applicable to other projects.\n\nSplit code into two stages\nAgent-based models and spatial interaction models require some kind of input. Often the effort to transform external data into this input can exceed that of the simulation component. Cleanly separating the two problems has some advantages:\n\niterate on the simulation faster, without processing raw data every run\nreuse the prepared input for future projects\nforce thinking about the data model needed by the simulation, and transform the external data into that form\n\n\n\nExplicit data schema\nDynamically typed languages like Python don’t force you to explicitly list the shape of input data. It’s common to read CSV files with pandas, filter and transform the data, and use that throughout the program. This can be quick to start prototyping, but is hard to maintain longer-term. Investing in the process of writing down types:\n\nmakes it easier for somebody new to understand your system – they can first focus on what you’re modeling, instead of how that’s built up from raw data sources\nclarifies what data actually matters to your system; you don’t carry forward unnecessary input\nmakes it impossible to express invalid states\n\nOne example is here – per person and activity, there’s a list of venues the person may visit, along with a probability of going there. If the list of venues and list of probabilities are stored as separate lists or columns, then their length may not match.\n\nreuse the prepared input for future projects\n\nThere’s a variety of techniques for expressing strongly typed data:\n\nprotocol buffers or flatbuffers\nJSON schemas\nPython data classes and optional type hints\nstatically typed languages like Rust\n\n\n\nType-safe IDs\nSay your data model has many different objects, each with their own ID – people, households, venues, etc. You might store these in a list and use the index as an ID. This is fine, but nothing stops you from confusing IDs and accidentally passing in venue 5 to a function instead of household 5. In Rust, it’s easy to create “wrapper types” like this and let the compiler prevent these mistakes.\nThis technique is also useful when preparing external data. GTFS data describing public transit routes and timetables contains many string IDs – shapes, trips, stops, routes. As soon as you read the raw input, you can store the strings in more precise types that prevent mixing up a stop ID and route ID.\n\n\nIdempotent data preparation\nIf you’re iterating on your initialisation pipeline’s code, you probably don’t want to download a 2GB external file every single run. A common approach is to first test if a file exists and don’t download it again if so. In practice, you may also need to handle unzipping files, showing a progress bar while downloading, and printing clear error messages. This codebase has some common code for doing this in Rust. We intend to publish a separate library to more easily call in your own code.\n\n\nLogging with structure\nIt’s typical to print information as a complex pipeline runs, for the user to track progress and debug problems. But without any sort of organization, it’s hard to follow what steps take a long time or break. What if your logs could show the logical structure of your pipeline and help you understand where time is spent?\n\nThe screenshot above shows a summary printed at the end of a long pipeline run. It’s immediately obvious that the slowest step is creating commuting flows.\nThis codebase uses the tracing framework for logging, with a custom piece to draw the tree. (We’ll publish this as a separate library once it’s more polished.) The tracing framework is hard to understand, but the main conceptual leap over regular logging framworks is the concept of a span. When your code starts one logical step, you call a method to create a new span, and when it finishes, you close that span. Spans can be nested in any way – create_commuting_flows happens within the larger step of creating population.\n\n\nDeterminism\nGiven the same inputs, your code should always produce identical output, no matter where it’s run or how many times. Otherwise, debugging problems becomes very tedious, and it’s more difficult to make conclusions from results. Of course, many projects have a stochastic element – but this should be controlled by a random number generator (RNG) seed, which is part of the input. You vary the seed and repeat the program, then reason about the distribution of results.\nAside from organizing your code to let a single RNG seed influence everything, another possible source of non-determinism is iteration order. In Rust, a HashMap could have different order every time it’s used, so we use a BTreeMap instead when this matters. In Python, dictionaries are ordered. Be sure to check for your language."
  },
  {
    "objectID": "code_walkthrough.html#protocol-buffers",
    "href": "code_walkthrough.html#protocol-buffers",
    "title": "Code walkthrough",
    "section": "Protocol buffers",
    "text": "Protocol buffers\nSPC uses protocol buffers for output. This has some advantages explained the “explicit data schema” section above, but the particular choice of protocol buffer has some limitations.\nFirst, proto3 doesn’t support required fields. This is done to allow schemas to evolve better over time, but this isn’t a feature SPC makes use of. There’s no need to have new code work with old data, or vice versa – if the schema is updated, downstream code should adapt accordingly and use the updated input files. The lack of required fields leads to imprecise code – a person’s health structure is always filled out, but in Rust, we wind up with Option<Health>. Differentiating 0 from missing data also becomes impossible – urn is optional, but in protobuf, we’re forced to map the missing case to 0 and document this.\nSecond, protocol buffers don’t easily support type-safe wrappers around numeric IDs, so downstream code has to be careful not to mix up household, venue, and person IDs.\nThird, protocol buffers support limited key types for maps. Enumerations can’t be used, so we use the numeric value for the activity enum.\nWe’ll evaluate flatbuffers and other alternative encodings.\nNote that in any case, SPC internally doesn’t use the auto-generated code until the very end of the pipeline. It’s always possible to be more precise with native Rust types, and convert to the less strict types last."
  },
  {
    "objectID": "running.html#initial-requirements",
    "href": "running.html#initial-requirements",
    "title": "Running SPC",
    "section": "Initial Requirements",
    "text": "Initial Requirements\n\nRust: The latest version of Rust (1.60): https://www.rust-lang.org/tools/install\nFor Mac users, we recommend to have installed Homebrew: https://brew.sh/"
  },
  {
    "objectID": "running.html#one-time-installation",
    "href": "running.html#one-time-installation",
    "title": "Running SPC",
    "section": "One-time installation",
    "text": "One-time installation\ngit clone https://github.com/alan-turing-institute/uatk-spc/\ncd uatk-spc\n# The next command will take a few minutes the first time you do it, to build external dependencies\ncargo build --release\nIf you get some errors during the compilation process, take a look at Troubleshooting section below."
  },
  {
    "objectID": "running.html#generating-output-for-a-study-area",
    "href": "running.html#generating-output-for-a-study-area",
    "title": "Running SPC",
    "section": "Generating output for a study area",
    "text": "Generating output for a study area\ncargo run --release -- config/west_yorkshire_small.txt\nThis will download some large files the first time. You’ll wind up with data/output/west_yorkshire_small.pb as output, as well as lots of intermediate files in data/raw_data/. The next time you run this command (even on a different study area), it should go much faster."
  },
  {
    "objectID": "running.html#adding-a-new-study-area",
    "href": "running.html#adding-a-new-study-area",
    "title": "Running SPC",
    "section": "Adding a new study area",
    "text": "Adding a new study area\nA study area requires a list of MSOAs to include. Create a new file config/your_region.txt with this list, following the format of the other files in there.\nYou can use the scripts/select_msoas.py script to generate this list based on an ONS geography code. The script looks for every MSOA where the CTY20NM is Liverpool. Refer to data/raw_data/referencedata/lookUp.csv for all geographies.\nAfter you write a new file, you simply run the pipeline with that as input:\ncargo run --release -- config/your_region.txt"
  },
  {
    "objectID": "running.html#docker",
    "href": "running.html#docker",
    "title": "Running SPC",
    "section": "Docker",
    "text": "Docker\nIf you’re having trouble building, you can run in Docker. Assuming you have Docker setup:\ngit clone https://github.com/alan-turing-institute/uatk-spc/\ncd spc\n# Build the Docker image initially. Once we publish to Docker Hub, this step\n# won't be necessary.\ndocker build -t spc .\n# Run SPC in Docker\ndocker run --mount type=bind,source=\"$(pwd)\"/data,target=/spc/data -t spc /spc/target/release/spc config/west_yorkshire_small.txt\nThis will make the data directory in your directory available to the Docker image, where it’ll download the large input files and produce the final output."
  },
  {
    "objectID": "running.html#troubleshooting",
    "href": "running.html#troubleshooting",
    "title": "Running SPC",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nPlease open an issue if you have any trouble!\n\nBuilding proj\nSymptom: When you run cargo build --release, you get an error like:\n error: failed to run custom build command for 'proj-sys v0.18.4'\nCause: The Rust code depends on proj to transform coordinates. You may need to install additional dependencies to build it.\nSuggested Solution\nOn Ubuntu, run:\napt-get install cmake sqlite3 libclang-dev\nOn Mac, run:\nbrew install pkg-config cmake proj\n\n\nDownloading\nIf you have trouble downloading any of the large files, you can download them manually. The logs will contain a line such as Downloading https://ramp0storage.blob.core.windows.net/nationaldata/QUANT_RAMP_spc.tar.gz to data/raw_data/nationaldata/QUANT_RAMP_spc.tar.gz. This tells you the URL to retrieve, and where to put the output file. Note that SPC won’t attempt to download files if they already exist, so if you wind up with a partially downloaded file, you have to manually remove it."
  },
  {
    "objectID": "performance.html",
    "href": "performance.html",
    "title": "Performance",
    "section": "",
    "text": "The following tables summarizes the resources SPC needs to run in different areas.\n\n\n\n\n\n\n\n\n\n\n\n\n\nstudy_area\nnum_msoas\nnum_households\nnum_people\npb_file_size\nruntime\ncommuting_runtime\nmemory_usage\n\n\n\n\nbedfordshire\n74\n271,487\n650,950\n94.35MiB\n10 seconds\n6 seconds\n295.78MiB\n\n\nberkshire\n107\n363,653\n878,045\n127.03MiB\n13 seconds\n7 seconds\n302.27MiB\n\n\nbristol\n55\n196,230\n456,532\n68.20MiB\n5 seconds\n1 second\n153.51MiB\n\n\nbuckinghamshire\n99\n324,843\n759,879\n109.15MiB\n10 seconds\n5 seconds\n299.43MiB\n\n\ncambridgeshire\n98\n346,532\n834,141\n120.21MiB\n13 seconds\n6 seconds\n302.25MiB\n\n\ncheshire\n139\n463,106\n1,040,634\n150.61MiB\n15 seconds\n6 seconds\n306.93MiB\n\n\ncornwall\n74\n246,873\n564,604\n83.34MiB\n7 seconds\n2 seconds\n279.33MiB\n\n\ncumbria\n64\n224,779\n485,035\n70.73MiB\n8 seconds\n3 seconds\n154.04MiB\n\n\nderbyshire\n131\n457,791\n1,024,952\n148.77MiB\n16 seconds\n8 seconds\n306.59MiB\n\n\ndevon\n156\n521,790\n1,178,315\n171.73MiB\n20 seconds\n10 seconds\n558.07MiB\n\n\ndorset\n95\n344,246\n751,334\n109.21MiB\n11 seconds\n4 seconds\n300.99MiB\n\n\ndurham\n117\n406,164\n904,785\n130.78MiB\n10 seconds\n3 seconds\n301.74MiB\n\n\neast_sussex\n102\n380,180\n830,761\n120.05MiB\n11 seconds\n5 seconds\n301.82MiB\n\n\neast_yorkshire_with_hull\n75\n261,267\n579,746\n85.40MiB\n7 seconds\n2 seconds\n280.82MiB\n\n\nessex\n211\n771,734\n1,798,893\n260.51MiB\n36 seconds\n24 seconds\n602.83MiB\n\n\ngloucestershire\n107\n392,120\n901,395\n129.72MiB\n13 seconds\n5 seconds\n304.15MiB\n\n\ngreater_manchester\n346\n871,651\n2,746,858\n388.45MiB\n2 minutes\n83 seconds\n1.09GiB\n\n\nhampshire\n225\n775,203\n1,803,991\n262.30MiB\n41 seconds\n28 seconds\n603.87MiB\n\n\nherefordshire\n23\n83,115\n191,282\n27.91MiB\n4 seconds\n1 second\n76.86MiB\n\n\nhertfordshire\n153\n492,783\n1,144,974\n165.25MiB\n19 seconds\n11 seconds\n557.76MiB\n\n\nisle_of_wight\n18\n64,602\n135,125\n20.72MiB\n3 seconds\n1 second\n72.07MiB\n\n\nkent\n220\n692,896\n1,808,206\n259.15MiB\n29 seconds\n18 seconds\n601.44MiB\n\n\nlancashire\n191\n603,524\n1,472,550\n210.57MiB\n22 seconds\n12 seconds\n594.20MiB\n\n\nleeds\n107\n333,449\n771,520\n112.94MiB\n21 seconds\n7 seconds\n301.40MiB\n\n\nleicestershire\n120\n417,621\n1,043,283\n147.66MiB\n17 seconds\n10 seconds\n304.30MiB\n\n\nlincolnshire\n134\n473,854\n1,064,174\n153.04MiB\n13 seconds\n6 seconds\n553.98MiB\n\n\nliverpool\n61\n218,559\n494,999\n70.81MiB\n9 seconds\n3 seconds\n152.88MiB\n\n\nlondon\n983\n3,135,814\n8,672,103\n1.23GiB\n11 minutes\n10 minutes\n4.32GiB\n\n\nmerseyside\n184\n546,791\n1,401,012\n199.91MiB\n25 seconds\n17 seconds\n592.73MiB\n\n\nnorfolk\n110\n379,188\n891,006\n129.99MiB\n11 seconds\n4 seconds\n304.41MiB\n\n\nnorth_yorkshire\n138\n434,489\n1,069,514\n154.35MiB\n14 seconds\n6 seconds\n554.40MiB\n\n\nnorthamptonshire\n91\n293,580\n733,190\n106.13MiB\n9 seconds\n4 seconds\n299.26MiB\n\n\nnorthumberland\n40\n113,436\n316,618\n45.45MiB\n5 seconds\n1 second\n141.78MiB\n\n\nnottinghamshire\n138\n413,097\n1,139,096\n163.42MiB\n18 seconds\n11 seconds\n555.30MiB\n\n\noxfordshire\n86\n254,974\n669,237\n96.24MiB\n9 seconds\n4 seconds\n281.55MiB\n\n\nrutland\n5\n16,688\n39,475\n5.79MiB\n2 seconds\n1 second\n20.63MiB\n\n\nshropshire\n62\n153,284\n497,064\n70.73MiB\n6 seconds\n2 seconds\n152.21MiB\n\n\nsomerset\n124\n384,165\n944,394\n137.94MiB\n14 seconds\n6 seconds\n304.68MiB\n\n\nsouth_yorkshire\n172\n358,717\n1,373,401\n191.22MiB\n25 seconds\n17 seconds\n556.05MiB\n\n\nstaffordshire\n143\n439,176\n1,104,925\n157.70MiB\n14 seconds\n6 seconds\n554.71MiB\n\n\nsuffolk\n90\n326,760\n739,296\n106.69MiB\n9 seconds\n3 seconds\n298.95MiB\n\n\nsurrey\n151\n461,466\n1,136,090\n163.70MiB\n21 seconds\n13 seconds\n557.62MiB\n\n\ntwo_counties\n4\n14,011\n31,024\n4.73MiB\n10 seconds\n1 second\n11.73MiB\n\n\ntyne_and_wear\n145\n414,128\n1,111,239\n157.14MiB\n12 seconds\n6 seconds\n553.16MiB\n\n\nwarwickshire\n108\n319,511\n933,391\n132.23MiB\n16 seconds\n9 seconds\n302.51MiB\n\n\nwest_midlands\n314\n523,264\n2,475,918\n347.98MiB\n37 seconds\n24 seconds\n1.05GiB\n\n\nwest_sussex\n100\n373,326\n838,440\n122.10MiB\n11 seconds\n4 seconds\n302.56MiB\n\n\nwest_yorkshire_large\n299\n960,426\n2,272,063\n331.00MiB\n41 seconds\n27 seconds\n1.08GiB\n\n\nwest_yorkshire_small\n3\n11,105\n27,466\n4.26MiB\n6 seconds\n1 second\n11.59MiB\n\n\nwiltshire\n89\n305,679\n686,963\n100.68MiB\n8 seconds\n2 seconds\n297.41MiB\n\n\nworcestershire\n85\n254,383\n572,751\n83.90MiB\n8 seconds\n3 seconds\n279.81MiB\n\n\n\nNotes:\n\npb_file_size refers to the size of the uncompressed protobuf file in data/output/\nThe total runtime is usually dominated by matching workers to businesses, so commuting_runtime gives a breakdown\nMeasuring memory usage of Linux processes isn’t straightforward, so memory_usage should just be a guide\nThese measurements were all taken on one developer’s laptop, and they don’t represent multiple runs. This table just aims to give a general sense of how long running takes.\n\nThat machine has 16 cores, which matters for the parallelized commuting calculation.\n\nscripts/collect_stats.py produces the table above"
  }
]