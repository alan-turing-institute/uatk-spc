[
  {
    "objectID": "understanding.html#understanding-the-schema",
    "href": "understanding.html#understanding-the-schema",
    "title": "The Data Schema",
    "section": "Understanding the schema",
    "text": "Understanding the schema\nHere are some helpful tips for understanding the schema.\nEach .pb file contains exactly one Population message. In contrast to datasets consisting of multiple .csv files, just a single file contains everything. Some of the fields in Population are lists (of people and households) or maps (of venues keyed by activity, or of MSOAs). Unlike a flat .csv table, there may be more lists embedded later. Each Household has a list of members, for example.\nThe different objects refer to each other, forming a graph structure. The protobuf uses uint64 IDs to index into other lists. For example, if some household has members = [3, 10], then those two people can be found at population.people[3] and population.people[10]. Each of them will have the same household ID, pointing back to something in the population.households list."
  },
  {
    "objectID": "understanding.html#modelling-the-daily-activites---flows",
    "href": "understanding.html#modelling-the-daily-activites---flows",
    "title": "The Data Schema",
    "section": "Modelling the daily activites - Flows",
    "text": "Modelling the daily activites - Flows\nSPC models daily travel behavior of people as “flows.” Flows are broken down by by an activity – shopping/retail, attending primary or secondary school, working, or staying at home. For each activity type, a person has a list of venues where they may do that activity, weighted by a probability of going to that particular venue.\nNote that flows_per_activity is stored in InfoPerMSOA, not Person. The flows for retail and school are only known at the MSOA level, not individually. So given a particular Person object, you first look up their household’s MSOA – msoa = population.households[ person.household ].msoa and then look up flows for that MSOA – population.info_per_msoa[msoa].flows_per_activity.\nEach person has exactly 1 flow for home – it’s just person.household with probability 1. A person has 0 or 1 flows to work, based on the value of person.workplace.\nThis doesn’t mean that all people in the same MSOA share the same travel behavior. Each person has their own activity_durations field, based on time-use survey data. Even if two people share the same set of places where they may go shopping, one person may spend much more time on that activity than another.\nSee the ASPICS conversion script for all of this in action – it has a function to collapse a person’s flows down into a single weighted list.\nHow do you interpret the probabilities/weights for flows? If your model needs people to visit specific places each day, you could randomly sample a venue from the flows, weighting them appropriately. For retail, you may want to repeat this sampling every day of the simulation, so they visit different venues. For primary and secondary school, it may be more appropriate to sample once and store that for the simulation – a student probably doesn’t switch schools daily.\nAlternatively, you can follow what ASPICS does. Every day, each person logically visits all possible venues, but their interaction there (possibly receiving or transmitting COVID) is weighted by the probability of each venue."
  },
  {
    "objectID": "modelling_methods.html#commuting-flows",
    "href": "modelling_methods.html#commuting-flows",
    "title": "Modelling methods",
    "section": "Commuting flows",
    "text": "Commuting flows\nIn order to distribute each individual of the population to a unique physical workplace, we first created a population of all individual workplaces in England, based on a combination of the Nomis UK Business Counts 2020 dataset and the Nomis Business register and Employment Survey 2015 (see Data sources). The first dataset gives the number of individual workplace counts per industry, using the SIC 2007 industry classification, with imprecise size (i.e. number of employees) bands at MSOA level. The second dataset gives the total number of jobs available at LSOA level per SIC 2007 industry category. We found that the distribution of workplace sizes follows closely a simple 1/x distribution, allowing us to draw for each workplace a size within their band, with sum constraints given by the total number of jobs available, according to the second dataset.\nThe workplace ‘population’ and individual population are then levelled for each SIC 2007 category by removing the exceeding part of whichever dataset lists more items. This takes into account that people and business companies are likely to over-report their working availability (e.g. part time and seasonal contracts are not counted differently than full time contracts, jobseekers or people on maternity leave might report the SIC of their last job). This process can be controlled by a threshold in the parameter file that defines the maximal total proportion of workers or jobs that can be removed. If the two datasets cannot be levelled accordingly, the categories are dropped and the datasets are levelled globally. Tests in the West Yorkshire area have shown than when the level 1 SIC, containing 21 unique categories, is used, 90% of the volume of commuting flows were recovered compared to the Nomis commuting OD matrices at MSOA level.\nThe employees for each workplace are drawn according to the ‘universal law of visitation’, see\n\nSchläpfer M et al. The universal visitation law of human mobility. Nature 593, 522–527 (2021). (DOI)\n\nThis framework predicts that visitors to any destination follow a simple\n\nρ(r,f)= K / (rf)2\n\ndistribution, where ρ(r,f) is the density of visitors coming from a distance r with frequency f and K is a balancing constant depending on the specific area. In the context of commuting, it can be assumed that f = 1. Additionally, we only need to weigh potential employees against each other, which removes the necessity to compute explicitly K. In the West Yorkshire test, we found a Pearson coefficient of 0.7 between the predicted flows when aggregated at MSOA level and the OD matrix at MSOA level available from Nomis."
  },
  {
    "objectID": "data_sources.html#utility-data",
    "href": "data_sources.html#utility-data",
    "title": "Data sources",
    "section": "Utility data",
    "text": "Utility data\nlookUp.csv\nThe look-up table links different geographies together. It is used internally by the model, but can also help the user define their own study area. MSOA11CD, MSOA11NM, LAD20CD, LAD20NM, ITL321CD, ITL321NM, ITL221CD, ITL221NM, ITL121CD, ITL121NM are all standard denominations fully compatible with ONS fields of the same name. They are based on ONS lookups. See ONS documentation for more details. CTY20NM and CCTY20NM are custom denominations for the counties of England (used to sort the county level population data) and the ceremonial counties of England respectively. Their spelling may vary in different data sources and the field CTY20NM is not compatible with the ONS field of the same name (which excludes all counties that are also unitary authorities). GoogleMob and OSM are different spellings for the counties of England used by Google and OSM for their data releases."
  },
  {
    "objectID": "data_sources.html#county-level-data",
    "href": "data_sources.html#county-level-data",
    "title": "Data sources",
    "section": "County level data",
    "text": "County level data\nContains 47 files, each representing the population in 2020 of one of the counties of England mentioned above, and named\ntus_hse_<county_name>.gz\nThis data is based on the 2011 UK census, the Time Use Survey 2014-15 and the Health Survey for England 2017. The SPENSER (Synthetic Population Estimation and Scenario Projection) microsimulation model (reference) distributes a synthetic population based on the census at MSOA scale and projects it to 2020 according to estimates from the Office for National Statistics (ONS). This information was enriched with some of the content of the other two datasets through propensity score matching (PSM) by Prof. Karyn Morrissey (Technical University of Denmark). The rest of the datasets can be added a posteriori from the identifiers provided.\nThe fields currently contained are:\n\nidp: a unique individual identifier within the present data\nMSOA11CD: MSOA code where the individual lives\nhid: household identifier, includes communal establishments\npid: identifier linking to the 2011 Census\npid_tus: identifier linking to the Time Use Survey 2015\npid_hse: identifier linking to the Health Survey for England 2017\nsex: 0 female; 1 male\nage: in years\norigin: 1 White; 2 Black; 3 Asian; 4 Mixed; 5 Other\nnssec5: National Statistics Socio-economic classification:\n\n1: Higher managerial, administrative and professional occupations\n2: Intermediate occupations\n3: Small employers and own account workers\n4: Lower supervisory and technical occupations\n5: Semi-routine and routine occupations\n0: Never worked and long-term unemployed\n\nsoc2010: Previous version of the Standard Occupational Classification\nsic1d07: Standard Industrial Classification of Economic Activities 2007, 1st layer (number corresponding to the letter in alphabetical order)\nsic2d07: Standard Industrial Classification of Economic Activities 2007, 2nd layer\nProportion of 24h spent doing different daily activities:\n\npunknown + pwork + pschool + pshop + pservices + pleisure + pescort + ptransport = pnothome\nphome + pworkhome = phometot\npnothome + phometot = 1\n\ncvd: has a cardio-vascular disease (0 or 1)\ndiabetes: has diabetes (0 or 1)\nbloodpressure: has high blood pressure (0 or 1)\nBMIvg6: Body Mass Index:\n\nNot applicable\nUnderweight: less than 18.5\nNormal: 18.5 to less than 25\nOverweight: 25 to less than 30\nObese I: 30 to less than 35\nObese II: 35 to less than 40\nObese III: 40 or more\n\nlng: longitude of the MSOA11CD centroid\nlat: latitude of the MSOA11CD centroid\n\nSome other fields were kept from specific projects but are not from official sources and should be used."
  },
  {
    "objectID": "data_sources.html#national-data",
    "href": "data_sources.html#national-data",
    "title": "Data sources",
    "section": "National data",
    "text": "National data\nbusinessRegistry.csv\nContains a breakdown of all business units (i.e. a single workplace) in England at LSOA scale (smaller than MSOA), estimated by the project contributors from two nomis datasets: UK Business Counts - local units by industry and employment size band 2020 and Business Register and Employment Survey 2015. Each item contains the size of the unit and its main sic1d07 code in reference to standard Industrial Classification of Economic Activities 2007 (number corresponding to the letter in alphabetical order). It is used to compute commuting flows.\nMSOAS_shp.tar.gz\nIs a simple shapefile taken from ONS boundaries.\nQUANT_RAMP.tar.gz\nSee: Milton R, Batty M, Dennett A, dedicated RAMP Spatial Interaction Model GitHub repository. It is used to compute the flows towards schools and retail.\ntimeAtHomeIncreaseCTY.csv\nThis file is a subset from Google COVID-19 Community Mobility Reports, cropped to England. It describes the daily reduction in mobility, averaged at county level, due to lockdown and other COVID-19 restrictions between the 15th of February 2020 and 15th of April 2022. Missing values have been replaced by the national average. These values can be used directly to reduce pnothome and increase phometot (and their sub-categories) to simulate more accurately the period."
  },
  {
    "objectID": "troubleshooting.html#docker",
    "href": "troubleshooting.html#docker",
    "title": "Troubleshooting",
    "section": "Docker",
    "text": "Docker\nIf you’re having trouble building, you can run in Docker. Assuming you have Docker setup:\ngit clone https://github.com/alan-turing-institute/uatk-spc/\ncd spc\n# Build the Docker image initially. Once we publish to Docker Hub, this step\n# won't be necessary.\ndocker build -t spc .\n# Run SPC in Docker\ndocker run --mount type=bind,source=\"$(pwd)\"/data,target=/spc/data -t spc /spc/target/release/spc config/west_yorkshire_small.txt\nThis will make the data directory in your directory available to the Docker image, where it’ll download the large input files and produce the final output."
  },
  {
    "objectID": "troubleshooting.html#building-proj",
    "href": "troubleshooting.html#building-proj",
    "title": "Troubleshooting",
    "section": "Building proj",
    "text": "Building proj\nSymptom: When you run cargo build --release, you get an error like:\n error: failed to run custom build command for 'proj-sys v0.18.4'\nCause: The Rust code depends on proj to transform coordinates. You may need to install additional dependencies to build it.\nSuggested Solution\nOn Ubuntu, run:\napt-get install cmake sqlite3 libclang-dev\nOn Mac, run:\nbrew install pkg-config cmake proj"
  },
  {
    "objectID": "troubleshooting.html#downloading",
    "href": "troubleshooting.html#downloading",
    "title": "Troubleshooting",
    "section": "Downloading",
    "text": "Downloading\nIf you have trouble downloading any of the large files, you can download them manually. The logs will contain a line such as Downloading https://ramp0storage.blob.core.windows.net/nationaldata/QUANT_RAMP_spc.tar.gz to data/raw_data/nationaldata/QUANT_RAMP_spc.tar.gz. This tells you the URL to retrieve, and where to put the output file. Note that SPC won’t attempt to download files if they already exist, so if you wind up with a partially downloaded file, you have to manually remove it."
  },
  {
    "objectID": "transform_it.html#python",
    "href": "transform_it.html#python",
    "title": "Transform the output file",
    "section": "Python",
    "text": "Python\nTo work with SPC protobufs in Python, you need two dependencies setup:\n\nThe protobuf library\n\nYou can install system-wide with pip install protobuf\nOr add as a dependency to a conda, poetry, etc environment\n\nThe generated Python library, synthpop_pb2.py\n\nYou can download a copy of this file into your codebase, then import synthpop_pb2\nYou can also generate the file yourself, following the docs: protoc --python_out=python/ synthpop.proto"
  },
  {
    "objectID": "transform_it.html#converting-.pb-file-to-json-format",
    "href": "transform_it.html#converting-.pb-file-to-json-format",
    "title": "Transform the output file",
    "section": "Converting .pb file to JSON format",
    "text": "Converting .pb file to JSON format\nTo interactively explore the data, viewing JSON is much easier. It shows the same structure as the protobuf, but in a human-readable text format. The example below uses a small Python script:\n# Download a file\nwget https://ramp0storage.blob.core.windows.net/spc-output/v1/rutland.pb.gz\n# Uncompress\ngunzip rutland.pb.gz\n# Convert the .pb to JSON\npython3 python/protobuf_to_json.py data/output/rutland.pb > rutland.json\n# View the output\nless rutland.json"
  },
  {
    "objectID": "transform_it.html#converting-to-numpy-arrays",
    "href": "transform_it.html#converting-to-numpy-arrays",
    "title": "Transform the output file",
    "section": "Converting to numpy arrays",
    "text": "Converting to numpy arrays\nThe ASPICS project simulates the spread of COVID through a population. The code uses numpy, and this script converts the protobuf to a bunch of different numpy arrays.\nNote the code doesn’t keep using classes from the generated Python code for protobufs. The protobuf is a format optimized for reading and writing; you shouldn’t use it in your model if there’s a more appropriate tool you’re familiar with, like data frames."
  },
  {
    "objectID": "transform_it.html#visualizing-the-output-files.",
    "href": "transform_it.html#visualizing-the-output-files.",
    "title": "Transform the output file",
    "section": "Visualizing the output files.",
    "text": "Visualizing the output files.\nUse this script to read a protobuf file, then draws a dot for every venue, color-coded by activity."
  },
  {
    "objectID": "code_walkthrough.html#generally-useful-techniques",
    "href": "code_walkthrough.html#generally-useful-techniques",
    "title": "Code walkthrough",
    "section": "Generally useful techniques",
    "text": "Generally useful techniques\nThe code-base makes use of some techniques that may be generally applicable to other projects.\n\nSplit code into two stages\nAgent-based models and spatial interaction models require some kind of input. Often the effort to transform external data into this input can exceed that of the simulation component. Cleanly separating the two problems has some advantages:\n\niterate on the simulation faster, without processing raw data every run\nreuse the prepared input for future projects\nforce thinking about the data model needed by the simulation, and transform the external data into that form\n\n\n\nExplicit data schema\nDynamically typed languages like Python don’t force you to explicitly list the shape of input data. It’s common to read CSV files with pandas, filter and transform the data, and use that throughout the program. This can be quick to start prototyping, but is hard to maintain longer-term. Investing in the process of writing down types:\n\nmakes it easier for somebody new to understand your system – they can first focus on what you’re modeling, instead of how that’s built up from raw data sources\nclarifies what data actually matters to your system; you don’t carry forward unnecessary input\nmakes it impossible to express invalid states\n\nOne example is here – per person and activity, there’s a list of venues the person may visit, along with a probability of going there. If the list of venues and list of probabilities are stored as separate lists or columns, then their length may not match.\n\nreuse the prepared input for future projects\n\nThere’s a variety of techniques for expressing strongly typed data:\n\nprotocol buffers or flatbuffers\nJSON schemas\nPython data classes and optional type hints\nstatically typed languages like Rust\n\n\n\nType-safe IDs\nSay your data model has many different objects, each with their own ID – people, households, venues, etc. You might store these in a list and use the index as an ID. This is fine, but nothing stops you from confusing IDs and accidentally passing in venue 5 to a function instead of household 5. In Rust, it’s easy to create “wrapper types” like this and let the compiler prevent these mistakes.\nThis technique is also useful when preparing external data. GTFS data describing public transit routes and timetables contains many string IDs – shapes, trips, stops, routes. As soon as you read the raw input, you can store the strings in more precise types that prevent mixing up a stop ID and route ID.\n\n\nIdempotent data preparation\nIf you’re iterating on your initialisation pipeline’s code, you probably don’t want to download a 2GB external file every single run. A common approach is to first test if a file exists and don’t download it again if so. In practice, you may also need to handle unzipping files, showing a progress bar while downloading, and printing clear error messages. This codebase has some common code for doing this in Rust. We intend to publish a separate library to more easily call in your own code.\n\n\nLogging with structure\nIt’s typical to print information as a complex pipeline runs, for the user to track progress and debug problems. But without any sort of organization, it’s hard to follow what steps take a long time or break. What if your logs could show the logical structure of your pipeline and help you understand where time is spent?\n\nThe screenshot above shows a summary printed at the end of a long pipeline run. It’s immediately obvious that the slowest step is creating commuting flows.\nThis codebase uses the tracing framework for logging, with a custom piece to draw the tree. (We’ll publish this as a separate library once it’s more polished.) The tracing framework is hard to understand, but the main conceptual leap over regular logging framworks is the concept of a span. When your code starts one logical step, you call a method to create a new span, and when it finishes, you close that span. Spans can be nested in any way – create_commuting_flows happens within the larger step of creating population.\n\n\nDeterminism\nGiven the same inputs, your code should always produce identical output, no matter where it’s run or how many times. Otherwise, debugging problems becomes very tedious, and it’s more difficult to make conclusions from results. Of course, many projects have a stochastic element – but this should be controlled by a random number generator (RNG) seed, which is part of the input. You vary the seed and repeat the program, then reason about the distribution of results.\nAside from organizing your code to let a single RNG seed influence everything, another possible source of non-determinism is iteration order. In Rust, a HashMap could have different order every time it’s used, so we use a BTreeMap instead when this matters. In Python, dictionaries are ordered. Be sure to check for your language."
  },
  {
    "objectID": "code_walkthrough.html#protocol-buffers",
    "href": "code_walkthrough.html#protocol-buffers",
    "title": "Code walkthrough",
    "section": "Protocol buffers",
    "text": "Protocol buffers\nSPC uses protocol buffers for output. This has some advantages explained the “explicit data schema” section above, but the particular choice of protocol buffer has some limitations.\nFirst, proto3 doesn’t support required fields. This is done to allow schemas to evolve better over time, but this isn’t a feature SPC makes use of. There’s no need to have new code work with old data, or vice versa – if the schema is updated, downstream code should adapt accordingly and use the updated input files. The lack of required fields leads to imprecise code – a person’s health structure is always filled out, but in Rust, we wind up with Option<Health>. Differentiating 0 from missing data also becomes impossible – urn is optional, but in protobuf, we’re forced to map the missing case to 0 and document this.\nSecond, protocol buffers don’t easily support type-safe wrappers around numeric IDs, so downstream code has to be careful not to mix up household, venue, and person IDs.\nThird, protocol buffers support limited key types for maps. Enumerations can’t be used, so we use the numeric value for the activity enum.\nWe’ll evaluate flatbuffers and other alternative encodings.\nNote that in any case, SPC internally doesn’t use the auto-generated code until the very end of the pipeline. It’s always possible to be more precise with native Rust types, and convert to the less strict types last."
  },
  {
    "objectID": "getting_started.html#what-spc-does",
    "href": "getting_started.html#what-spc-does",
    "title": "Getting started",
    "section": "What SPC does?",
    "text": "What SPC does?\nThe SPC generates spatially enriched synthetic population outputs for multiple administrative and census boundaries including Local Authority Districts - LADs and Counties across England. The output file generated by SPC has a granularity of Middle Layer Super Output Areas - MSOAs. This file is estructured and convided to help other researchers or urban analysts to feed other dynamic models for multiples propouses where a enriched sythetic population file is requiered. SPC includes an comprenhenvise set of variables that include social-demogragfics characterictis, dayliy activities and other extra data sources to help you model the complexity of British society.\nFor information about the data sources and the varibles integrated in the SPC outfile, visit data sources. For information about the administrative and census boundaries included in SPC visit the Open Geography portal from the Office for National Statistics (ONS)."
  },
  {
    "objectID": "getting_started.html#what-is-the-output-generated-by-spc",
    "href": "getting_started.html#what-is-the-output-generated-by-spc",
    "title": "Getting started",
    "section": "What is the output generated by SPC?",
    "text": "What is the output generated by SPC?\nSPC creates Google’s protocol buffer files a platform-neutral, extensible mechanism for serializing structured data in bi-directional way. SPC encodes the enriched synthetic population schema in .proto files (.pb). You can read the “protobuf” (shorthand for a protocol buffer file) in any supported language, and then extract and transform just the parts of the data you want.\nNote that per MSOA, very few venues are represented as destinations – 10 for retail and 5 for school. Only the most likely venues from QUANT are used.\nSee the ASPICS conversion script for all of this in action – it has a function to collapse a person’s flows down into a single weighted list."
  },
  {
    "objectID": "getting_started.html#areas-ready-to-use-and-explore",
    "href": "getting_started.html#areas-ready-to-use-and-explore",
    "title": "Getting started",
    "section": "Areas ready to use and explore",
    "text": "Areas ready to use and explore\nIf all you need is to explore the enriched syntetic population file and mesuare if fits to you model, we have created the output file for all counties in England, so if the area you want to model is listed or you just want to explore the data, no need to run SPC yourself – just download data for your area.\nIf you are interested in create another area or combination of MSOAs, you will need then install and run SPC in your environment."
  },
  {
    "objectID": "using_spc.html#adding-a-new-area-text-file-in-config-folder",
    "href": "using_spc.html#adding-a-new-area-text-file-in-config-folder",
    "title": "Creating new areas",
    "section": "Adding a new area text file in /config folder",
    "text": "Adding a new area text file in /config folder\nSPC requieres a list of MSOAs for a given area. The MSOA code is not a conventional way to describe an area, so we have created a python script to assist you create the requiered input file for SPC.\nNavigate to the folder scripts within SPC folder, and run python select_msoas.py, this script will ask you the name of the LAD or County you are interest in. Refer to data/raw_data/referencedata/lookUp.csv for all geographies that SPC support.\nA new file will be created in your config folder config/your_region.txt with the list of MSOAs."
  },
  {
    "objectID": "using_spc.html#run-spc-for-the-new-area-list",
    "href": "using_spc.html#run-spc-for-the-new-area-list",
    "title": "Creating new areas",
    "section": "Run SPC for the new area list",
    "text": "Run SPC for the new area list\nAfter you write a new file, you simply run the pipeline with that as input, make sure you are located in the spc folder\ncargo run --release -- config/your_region.txt\nIf you like to take a quick look of how SPC process the data, you can use one of our small examples lists (west_yorkshire_small.txt, two_counties.txt)\ncargo run --release -- config/west_yorkshire_small.txt\nThis will download some large files the first time. You’ll wind up with data/output/west_yorkshire_small.pb as output, as well as lots of intermediate files in data/raw_data/. The next time you run this command (even on a different study area), it should go much faster."
  },
  {
    "objectID": "performance.html",
    "href": "performance.html",
    "title": "Performance",
    "section": "",
    "text": "The following tables summarizes the resources SPC needs to run in different areas.\n\n\n\n\n\n\n\n\n\n\n\n\n\nstudy_area\nnum_msoas\nnum_households\nnum_people\npb_file_size\nruntime\ncommuting_runtime\nmemory_usage\n\n\n\n\nbedfordshire\n74\n271,487\n650,950\n94.33MiB\n11 seconds\n6 seconds\n293.76MiB\n\n\nberkshire\n107\n363,653\n878,045\n127.21MiB\n13 seconds\n7 seconds\n300.25MiB\n\n\nbristol\n55\n196,230\n456,532\n68.11MiB\n5 seconds\n2 seconds\n151.48MiB\n\n\nbuckinghamshire\n99\n324,843\n759,879\n109.30MiB\n10 seconds\n5 seconds\n297.42MiB\n\n\ncambridgeshire\n98\n346,532\n834,141\n120.30MiB\n13 seconds\n7 seconds\n300.23MiB\n\n\ncheshire\n139\n463,106\n1,040,634\n150.80MiB\n15 seconds\n6 seconds\n304.91MiB\n\n\ncornwall\n74\n246,873\n564,604\n83.30MiB\n7 seconds\n2 seconds\n277.31MiB\n\n\ncumbria\n64\n224,779\n485,035\n70.69MiB\n8 seconds\n3 seconds\n152.02MiB\n\n\nderbyshire\n131\n457,791\n1,024,952\n148.85MiB\n16 seconds\n9 seconds\n304.58MiB\n\n\ndevon\n156\n521,790\n1,178,315\n171.93MiB\n21 seconds\n10 seconds\n556.07MiB\n\n\ndorset\n95\n344,246\n751,334\n109.25MiB\n11 seconds\n4 seconds\n298.97MiB\n\n\ndurham\n117\n406,164\n904,785\n130.75MiB\n10 seconds\n4 seconds\n299.72MiB\n\n\neast_sussex\n102\n380,180\n830,761\n120.14MiB\n12 seconds\n5 seconds\n299.80MiB\n\n\neast_yorkshire_with_hull\n75\n261,267\n579,746\n85.31MiB\n8 seconds\n2 seconds\n278.80MiB\n\n\nessex\n211\n771,734\n1,798,893\n261.01MiB\n40 seconds\n28 seconds\n600.82MiB\n\n\ngloucestershire\n107\n392,120\n901,395\n129.84MiB\n14 seconds\n5 seconds\n302.14MiB\n\n\ngreater_london\n983\n3,135,814\n8,672,103\n1.23GiB\n13 minutes\n12 minutes\n4.32GiB\n\n\ngreater_manchester\n346\n871,651\n2,746,858\n389.21MiB\n2 minutes\n86 seconds\n1.08GiB\n\n\nhampshire\n225\n775,203\n1,803,991\n262.78MiB\n41 seconds\n29 seconds\n601.86MiB\n\n\nherefordshire\n23\n83,115\n191,282\n27.72MiB\n4 seconds\n1 second\n74.83MiB\n\n\nhertfordshire\n153\n492,783\n1,144,974\n165.59MiB\n19 seconds\n11 seconds\n555.75MiB\n\n\nisle_of_wight\n18\n64,602\n135,125\n20.48MiB\n3 seconds\n1 second\n70.05MiB\n\n\nkent\n220\n692,896\n1,808,206\n259.58MiB\n30 seconds\n19 seconds\n599.43MiB\n\n\nlancashire\n191\n603,524\n1,472,550\n210.80MiB\n23 seconds\n12 seconds\n592.20MiB\n\n\nleeds\n107\n333,449\n771,520\n112.97MiB\n20 seconds\n7 seconds\n299.38MiB\n\n\nleicestershire\n120\n417,621\n1,043,283\n147.79MiB\n18 seconds\n11 seconds\n302.29MiB\n\n\nlincolnshire\n134\n473,854\n1,064,174\n153.14MiB\n14 seconds\n6 seconds\n551.97MiB\n\n\nliverpool\n61\n218,559\n494,999\n70.67MiB\n10 seconds\n3 seconds\n150.85MiB\n\n\nmerseyside\n184\n546,791\n1,401,012\n200.02MiB\n26 seconds\n17 seconds\n590.71MiB\n\n\nnorfolk\n110\n379,188\n891,006\n130.04MiB\n12 seconds\n4 seconds\n302.39MiB\n\n\nnorth_yorkshire\n138\n434,489\n1,069,514\n154.54MiB\n15 seconds\n6 seconds\n552.40MiB\n\n\nnorthamptonshire\n91\n293,580\n733,190\n106.21MiB\n10 seconds\n4 seconds\n297.24MiB\n\n\nnorthumberland\n40\n113,436\n316,618\n45.27MiB\n5 seconds\n1 second\n139.76MiB\n\n\nnottinghamshire\n138\n413,097\n1,139,096\n163.50MiB\n20 seconds\n12 seconds\n553.29MiB\n\n\noxfordshire\n86\n254,974\n669,237\n96.27MiB\n9 seconds\n4 seconds\n279.53MiB\n\n\nrutland\n5\n16,688\n39,475\n5.52MiB\n2 seconds\n1 second\n18.60MiB\n\n\nshropshire\n62\n153,284\n497,064\n70.66MiB\n6 seconds\n2 seconds\n150.20MiB\n\n\nsomerset\n124\n384,165\n944,394\n138.07MiB\n15 seconds\n7 seconds\n302.66MiB\n\n\nsouth_yorkshire\n172\n358,717\n1,373,401\n191.33MiB\n26 seconds\n18 seconds\n554.04MiB\n\n\nstaffordshire\n143\n439,176\n1,104,925\n157.81MiB\n15 seconds\n7 seconds\n552.70MiB\n\n\nsuffolk\n90\n326,760\n739,296\n106.71MiB\n10 seconds\n4 seconds\n296.93MiB\n\n\nsurrey\n151\n461,466\n1,136,090\n164.06MiB\n22 seconds\n14 seconds\n555.61MiB\n\n\ntwo_counties\n4\n14,011\n31,024\n4.46MiB\n10 seconds\n1 second\n9.70MiB\n\n\ntyne_and_wear\n145\n414,128\n1,111,239\n157.15MiB\n13 seconds\n6 seconds\n551.14MiB\n\n\nwarwickshire\n108\n319,511\n933,391\n132.32MiB\n17 seconds\n10 seconds\n300.49MiB\n\n\nwest_midlands\n314\n523,264\n2,475,918\n348.50MiB\n40 seconds\n26 seconds\n1.05GiB\n\n\nwest_sussex\n100\n373,326\n838,440\n122.18MiB\n11 seconds\n4 seconds\n300.54MiB\n\n\nwest_yorkshire_large\n299\n960,426\n2,272,063\n331.50MiB\n43 seconds\n28 seconds\n1.08GiB\n\n\nwest_yorkshire_small\n3\n11,105\n27,466\n3.97MiB\n7 seconds\n1 second\n9.56MiB\n\n\nwiltshire\n89\n305,679\n686,963\n100.69MiB\n8 seconds\n3 seconds\n295.39MiB\n\n\nworcestershire\n85\n254,383\n572,751\n83.91MiB\n9 seconds\n4 seconds\n277.79MiB\n\n\n\nNotes:\n\npb_file_size refers to the size of the uncompressed protobuf file in data/output/\nThe total runtime is usually dominated by matching workers to businesses, so commuting_runtime gives a breakdown\nMeasuring memory usage of Linux processes isn’t straightforward, so memory_usage should just be a guide\nThese measurements were all taken on one developer’s laptop, and they don’t represent multiple runs. This table just aims to give a general sense of how long running takes.\n\nThat machine has 16 cores, which matters for the parallelized commuting calculation.\n\nscripts/collect_stats.py produces the table above"
  },
  {
    "objectID": "developer_guide.html#updating-the-docs",
    "href": "developer_guide.html#updating-the-docs",
    "title": "Developer guide",
    "section": "Updating the docs",
    "text": "Updating the docs\nThe site is built with Quarto. You can iterate on it locally: cd docs; quarto preview"
  },
  {
    "objectID": "developer_guide.html#code-hygiene",
    "href": "developer_guide.html#code-hygiene",
    "title": "Developer guide",
    "section": "Code hygiene",
    "text": "Code hygiene\nWe use automated tools to format the code.\ncargo fmt\n\n# Format Markdown docs\nprettier --write *.md docs/*.md\nInstall prettier for Markdown."
  },
  {
    "objectID": "developer_guide.html#some-tips-for-working-with-rust",
    "href": "developer_guide.html#some-tips-for-working-with-rust",
    "title": "Developer guide",
    "section": "Some tips for working with Rust",
    "text": "Some tips for working with Rust\nThere are two equivalent ways to rebuild and then run the code. First:\ncargo run --release -- devon\nThe -- separates arguments to cargo, the Rust build tool, and arguments to the program itself. The second way:\ncargo build --release\n./target/release/aspics devon\nYou can build the code in two ways – debug and release. There’s a simple tradeoff – debug mode is fast to build, but slow to run. Release mode is slow to build, but fast to run. For the ASPICS codebase, since the input data is so large and the codebase so small, I’d recommend always using --release. If you want to use debug mode, just omit the flag.\nIf you’re working on the Rust code outside of an IDE like VSCode, then you can check if the code compiles much faster by doing cargo check."
  },
  {
    "objectID": "outputs.html#versioning",
    "href": "outputs.html#versioning",
    "title": "Outputs for England Counties",
    "section": "Versioning",
    "text": "Versioning\nOver time, we may add more data to SPC or change the schema. Protocol buffers are designed to let combinations of new/old code and data files work together, but we don’t intend to use this feature. We may make breaking changes, like deleting fields. We’ll release a new version of the schema and output data every time and document it here. You should depend on a specific version of the data output in your code, so new releases don’t affect you until you decide to update.\n\nv1: released 25/04/2022, schema"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "The Synthetic Population Catalyst (SPC) makes it easier for researchers to work with synthetic population data in England. It combines a variety of data sources and outputs a single file in protocol buffer format, describing the population in a given study area. The tool also provides methods to export the outcome in diferent formats often use for researchers like CSV or JSON.\nThe input of the SPC tool is a list of the Middle Layer Super Output Area (MSOAs) where you want to create a spatially enriched sythetic population to feed other dynamic models. SPC includes a script to assist you with the proper list of the MSOAs by defining a Local Authority District area in England. Get started to download SPC data or run the tool in different MSOAs."
  },
  {
    "objectID": "installation.html#initial-requirements",
    "href": "installation.html#initial-requirements",
    "title": "Installation",
    "section": "Initial Requirements",
    "text": "Initial Requirements\n\nRust: The latest version of Rust (1.60): https://www.rust-lang.org/tools/install\nFor Mac users, we recommend to have installed Homebrew: https://brew.sh/\nPython, there are some scrips and functionalites inside SPC that requiere python. https://www.python.org/downloads/"
  },
  {
    "objectID": "installation.html#one-time-installation",
    "href": "installation.html#one-time-installation",
    "title": "Installation",
    "section": "One-time installation",
    "text": "One-time installation\ngit clone https://github.com/alan-turing-institute/uatk-spc/\ncd uatk-spc\n# The next command will take a few minutes the first time you do it, to build external dependencies\ncargo build --release\nIf you get some errors during the compilation process, take a look at Troubleshooting section."
  }
]