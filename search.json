[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Synthetic Population Catalyst",
    "section": "",
    "text": "The Synthetic Population Catalyst (SPC) makes it easier for researchers to work with synthetic population data in England. It combines a variety of data sources and outputs a single file in protocol buffer format, describing the population in a given study area. The data includes demographic, health, and daily activity data per person, and information about the venues where people conduct activities.\nYou can use SPC output to catalyze your own project. Rather than join together many raw data sources yourself and deal with missing and messy data, you can leverage SPC’s effort and well-documented schema.\nTo get started:\n\nDownload sample data for a county in England\nExplore how to use the data\nIf you need a different study area, build and then run SPC\n\nYou can also download this site as a PDF and find all code on Github.\nThis work is supported by Wave 1 of The UKRI Strategic Priorities Fund under the EPSRC Grant EP/W006022/1, particularly the “Ecosystem of Digital Twins” theme within that grant and The Alan Turing Institute."
  },
  {
    "objectID": "outputs.html#versioning",
    "href": "outputs.html#versioning",
    "title": "2  Outputs for England Counties",
    "section": "2.1 Versioning",
    "text": "2.1 Versioning\nOver time, we may add more data to SPC or change the schema. Protocol buffers are designed to let combinations of new/old code and data files work together, but we don’t intend to use this feature. We may make breaking changes, like deleting fields. We’ll release a new version of the schema and output data every time and document it here. You should depend on a specific version of the data output in your code, so new releases don’t affect you until you decide to update.\n\nv1: released 25/04/2022, schema"
  },
  {
    "objectID": "use_output.html#python",
    "href": "use_output.html#python",
    "title": "3  Using the SPC output file",
    "section": "3.1 Python",
    "text": "3.1 Python\nTo work with SPC protobufs in Python, you need two dependencies setup:\n\nThe protobuf library\n\nYou can install system-wide with pip install protobuf\nOr add as a dependency to a conda, poetry, etc environment\n\nThe generated Python library, synthpop_pb2.py\n\nYou can download a copy of this file into your codebase, then import synthpop_pb2\nYou can also generate the file yourself, following the docs: protoc --python_out=python/ synthpop.proto\n\n\n\n3.1.1 Converting .pb file to JSON format\nTo interactively explore the data, viewing JSON is much easier. It shows the same structure as the protobuf, but in a human-readable text format. The example below uses a small Python script:\n# Download a file\nwget https://ramp0storage.blob.core.windows.net/spc-output/v1/rutland.pb.gz\n# Uncompress\ngunzip rutland.pb.gz\n# Convert the .pb to JSON\npython3 python/protobuf_to_json.py data/output/rutland.pb > rutland.json\n# View the output\nless rutland.json\n\n\n3.1.2 Converting to numpy arrays\nThe ASPICS project simulates the spread of COVID through a population. The code uses numpy, and this script converts the protobuf to a bunch of different numpy arrays.\nNote the code doesn’t keep using classes from the generated Python code for protobufs. The protobuf is a format optimized for reading and writing; you shouldn’t use it in your model if there’s a more appropriate tool you’re familiar with, like data frames.\n\n\n3.1.3 Visualizing venues\nUse this script to read a protobuf file, then draws a dot for every venue, color-coded by activity."
  },
  {
    "objectID": "installation.html#dependencies",
    "href": "installation.html#dependencies",
    "title": "4  Installation",
    "section": "4.1 Dependencies",
    "text": "4.1 Dependencies\n\nRust: The latest version of Rust (1.60): https://www.rust-lang.org/tools/install\nA build environment for proj, to transform coordinates.\n\nOn Ubuntu, run apt-get install cmake sqlite3 libclang-dev\nOn Mac, install Homebrew and run brew install pkg-config cmake proj"
  },
  {
    "objectID": "installation.html#compiling-spc",
    "href": "installation.html#compiling-spc",
    "title": "4  Installation",
    "section": "4.2 Compiling SPC",
    "text": "4.2 Compiling SPC\ngit clone https://github.com/alan-turing-institute/uatk-spc/\ncd uatk-spc\n# The next command will take a few minutes the first time you do it, to build external dependencies\ncargo build --release\nIf you get error: failed to run custom build command for 'proj-sys v0.18.4', then you’re likely missing dependencies listed above. Please open an issue if you have any trouble."
  },
  {
    "objectID": "installation.html#troubleshooting-downloading",
    "href": "installation.html#troubleshooting-downloading",
    "title": "4  Installation",
    "section": "4.3 Troubleshooting downloading",
    "text": "4.3 Troubleshooting downloading\nIf you get an error No such file or directory (os error 2) it might be because a previous attempt to run SPC failed, and some necessary files were not fully downloaded. In these cases you could try deleting the data/raw_data directory and then running SPC again. It should automatically try to download the big files again.\nIf you have trouble downloading any of the large files, you can download them manually. The logs will contain a line such as Downloading https://ramp0storage.blob.core.windows.net/nationaldata/QUANT_RAMP_spc.tar.gz to data/raw_data/nationaldata/QUANT_RAMP_spc.tar.gz. This tells you the URL to retrieve, and where to put the output file. Note that SPC won’t attempt to download files if they already exist, so if you wind up with a partially downloaded file, you have to manually remove it."
  },
  {
    "objectID": "custom_areas.html#specifying-the-area",
    "href": "custom_areas.html#specifying-the-area",
    "title": "5  Creating new study areas",
    "section": "5.1 Specifying the area",
    "text": "5.1 Specifying the area\nSPC takes a newline-separated list of MSOAs in the config/ directory as input, like this. You can generate this list from a LAD (local authority district). From the main SPC directory, run python scripts/select_msoas.py. Refer to data/raw_data/referencedata/lookUp.csv (only available after running SPC once) for all geographies available.\nThis script will create a new file, config/your_region.txt."
  },
  {
    "objectID": "custom_areas.html#run-spc-for-the-new-area",
    "href": "custom_areas.html#run-spc-for-the-new-area",
    "title": "5  Creating new study areas",
    "section": "5.2 Run SPC for the new area",
    "text": "5.2 Run SPC for the new area\nFrom the main directory, just run:\ncargo run --release -- config/your_region.txt\nThis will download some large files the first time. You’ll wind up with data/output/your_region.pb as output, as well as lots of intermediate files in data/raw_data/. The next time you run this command (even on a different study area), it should go much faster."
  },
  {
    "objectID": "custom_areas.html#optional-run-spc-for-lots-of-areas",
    "href": "custom_areas.html#optional-run-spc-for-lots-of-areas",
    "title": "5  Creating new study areas",
    "section": "5.3 (Optional) run SPC for lots of areas",
    "text": "5.3 (Optional) run SPC for lots of areas\nIf you want to run the program over lots of areas at once and are using mac/linux you can use a for loop in a terminal to repeatedly run SPC over all files in the config directory. For example, this will run SPC on all .txt files in the config directory:\nfor file in config/*.csv; do cargo run --release -- config/$file; done"
  },
  {
    "objectID": "schema.html#understanding-the-schema",
    "href": "schema.html#understanding-the-schema",
    "title": "6  Data schema",
    "section": "6.1 Understanding the schema",
    "text": "6.1 Understanding the schema\nHere are some helpful tips for understanding the schema.\nEach .pb file contains exactly one Population message. In contrast to datasets consisting of multiple .csv files, just a single file contains everything. Some of the fields in Population are lists (of people and households) or maps (of venues keyed by activity, or of MSOAs). Unlike a flat .csv table, there may be more lists embedded later. Each Household has a list of members, for example.\nThe different objects refer to each other, forming a graph structure. The protobuf uses uint64 IDs to index into other lists. For example, if some household has members = [3, 10], then those two people can be found at population.people[3] and population.people[10]. Each of them will have the same household ID, pointing back to something in the population.households list."
  },
  {
    "objectID": "schema.html#flows-modelling-daily-activites",
    "href": "schema.html#flows-modelling-daily-activites",
    "title": "6  Data schema",
    "section": "6.2 Flows: modelling daily activites",
    "text": "6.2 Flows: modelling daily activites\nSPC models daily travel behavior of people as “flows.” Flows are broken down by by an activity – shopping/retail, attending primary or secondary school, working, or staying at home. For each activity type, a person has a list of venues where they may do that activity, weighted by a probability of going to that particular venue.\nNote that flows_per_activity is stored in InfoPerMSOA, not Person. The flows for retail and school are only known at the MSOA level, not individually. So given a particular Person object, you first look up their household’s MSOA – msoa = population.households[ person.household ].msoa and then look up flows for that MSOA – population.info_per_msoa[msoa].flows_per_activity.\nEach person has exactly 1 flow for home – it’s just person.household with probability 1. A person has 0 or 1 flows to work, based on the value of person.workplace.\nThis doesn’t mean that all people in the same MSOA share the same travel behavior. Each person has their own activity_durations field, based on time-use survey data. Even if two people share the same set of places where they may go shopping, one person may spend much more time on that activity than another.\nSee the ASPICS conversion script for all of this in action – it has a function to collapse a person’s flows down into a single weighted list.\nNote that per MSOA, very few venues are represented as destinations – 10 for retail and 5 for school. Only the most likely venues from QUANT are used."
  },
  {
    "objectID": "schema.html#flow-weights",
    "href": "schema.html#flow-weights",
    "title": "6  Data schema",
    "section": "6.3 Flow weights",
    "text": "6.3 Flow weights\nHow do you interpret the probabilities/weights for flows? If your model needs people to visit specific places each day, you could randomly sample a venue from the flows, weighting them appropriately. For retail, you may want to repeat this sampling every day of the simulation, so they visit different venues. For primary and secondary school, it may be more appropriate to sample once and store that for the simulation – a student probably doesn’t switch schools daily.\nAlternatively, you can follow what ASPICS does. Every day, each person logically visits all possible venues, but their interaction there (possibly receiving or transmitting COVID) is weighted by the probability of each venue."
  },
  {
    "objectID": "modelling_methods.html#commuting-flows",
    "href": "modelling_methods.html#commuting-flows",
    "title": "7  Modelling methods",
    "section": "7.1 Commuting flows",
    "text": "7.1 Commuting flows\nIn order to distribute each individual of the population to a unique physical workplace, we first created a population of all individual workplaces in England, based on a combination of the Nomis UK Business Counts 2020 dataset and the Nomis Business register and Employment Survey 2015 (see Data sources). The first dataset gives the number of individual workplace counts per industry, using the SIC 2007 industry classification, with imprecise size (i.e. number of employees) bands at MSOA level. The second dataset gives the total number of jobs available at LSOA level per SIC 2007 industry category. We found that the distribution of workplace sizes follows closely a simple 1/x distribution, allowing us to draw for each workplace a size within their band, with sum constraints given by the total number of jobs available, according to the second dataset.\nThe workplace ‘population’ and individual population are then levelled for each SIC 2007 category by removing the exceeding part of whichever dataset lists more items. This takes into account that people and business companies are likely to over-report their working availability (e.g. part time and seasonal contracts are not counted differently than full time contracts, jobseekers or people on maternity leave might report the SIC of their last job). This process can be controlled by a threshold in the parameter file that defines the maximal total proportion of workers or jobs that can be removed. If the two datasets cannot be levelled accordingly, the categories are dropped and the datasets are levelled globally. Tests in the West Yorkshire area have shown than when the level 1 SIC, containing 21 unique categories, is used, 90% of the volume of commuting flows were recovered compared to the Nomis commuting OD matrices at MSOA level.\nThe employees for each workplace are drawn according to the ‘universal law of visitation’, see\n\nSchläpfer M et al. The universal visitation law of human mobility. Nature 593, 522–527 (2021). (DOI)\n\nThis framework predicts that visitors to any destination follow a simple\n\nρ(r,f)= K / (rf)2\n\ndistribution, where ρ(r,f) is the density of visitors coming from a distance r with frequency f and K is a balancing constant depending on the specific area. In the context of commuting, it can be assumed that f = 1. Additionally, we only need to weigh potential employees against each other, which removes the necessity to compute explicitly K. In the West Yorkshire test, we found a Pearson coefficient of 0.7 between the predicted flows when aggregated at MSOA level and the OD matrix at MSOA level available from Nomis."
  },
  {
    "objectID": "modelling_methods.html#income-data",
    "href": "modelling_methods.html#income-data",
    "title": "7  Modelling methods",
    "section": "7.2 Income data",
    "text": "7.2 Income data\nThis modelling is based on the 2020 revised edition of the Earnings and hours worked, region by occupation by four-digit SOC: ASHE Table 15 database from ONS. Some percentiles for employees’ gross hourly salaries are provided for each full-time and part-time job according to their four-digit SOC classification per region, and separated by sex.\n\n7.2.1 Methods\nThe data are far from complete (only about 15% of all possible values), especially for the highest deciles. We found that an order 3 polynomial fit was satisfactory for most categories (93.11%) to complete the partially filled SOCs. SOCs with too many missing values are given the value for the category that is immediately higher in the SOC hierarchy. Some jobs appear to have a ‘ceiling’ for the highest percentiles, making the polynomial fit fail. In that case, we have replaced the unknown values by the highest known value in the raw data (as there is no clear and systemic fit for these special cases). In addition, there is no information for the highest decile in all cases, which means that the highest salaries are underestimated (and exceptionally high salaries cannot be obtained). The result of this phase is four tables {male full-time, male part-time, female full-time, female part-time} containing the coefficients of the fitted order 3 polynomial, with an optional ceiling percentile when relevant.\nA percentile is chosen randomly (uniformly) for each individual, and the salary is then deduced according to their full-time/part-time status, region, sex and SOC category. A basic hourly salary column is added to the unprocessed SPC data, as well as a corresponding annual salary based on their estimated hours worked per day, according to the Time Use Survey matching. In addition, we repeat this process for all individuals that are categorised as ‘Self-employed’ or ‘Employee unspecified’ by the Time Use Survey matching as if they were full time employees. These values are recorded in the columns IncomeHAsIF and IncomeYAsIf. We noticed that a high number of employees were given no worked hours by the Time Use Survey. We have added to the IncomeYAsIf column an estimation of the annual salry based on Table 15.9a: Paid hours worked - Total 2020 and also depending on the same four variable as above (full-time/part-time status, region, sex and SOC category).\nThe R codes for this modelling are here.\nThe methods are validated in the next section. Since it is not possible to optimise every criterion at once, this next secton can also be used as a reference to re-adjust some values to match exactly the ONS estimated means for one particular criterion of interest.\n\n\n7.2.2 Comparison to reference values from ONS\nWe compare the results of the modelling to the raw datasets from ONS. - Mod for modelled - M for male - F for female - H for hourly gross salary - Y for annual gross salary - FT for Full-Time - PT for Part-Time Only individuals recorded as employees (i.e. not self-employed) are taken into account in this section.\nNumber of employees per sex and full-time/part-time classification\nThe numbers given by ONS vary from dataset to dataset and are reported by ONS as indicative only. For the modelled values, we give the total number of individuals with a non-zero salary in each category.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nAll\nFT\nPT\nM\nM FT\nM PT\nF\nF FT\nF PT\n\n\n\n\nTot\n22-26k\n16-19k\n6-8k\n11-13k\n9-11k\n1.5-2k\n11-13k\n6.5-7.5k\n4.5-5.5k\n\n\nMod tot H\n23.1k\n18.5k\n4.6k\n11.8k\n11k\n0.8k\n11.3k\n7.5k\n3.8k\n\n\nMod tot Y\n17.6k\n14.8k\n2.8k\n9.4k\n8.9k\n0.5k\n8.2k\n5.9k\n2.3k\n\n\n\nA significant number of individuals listed as working either full or part time have 0 effective worked hours per day according to the Time Use Survey matching. In those cases, an hourly salary is modelled depending on their SOC, region and sex, as for any other employee, but the annual salary will be displayed as 0. It is possible to estimate their likely true number of hours worked from the same ONS dataset (Table 15.9a: Paid hours worked - Total 2020), also depending on their sex, soc and region. This calculation has been added to the “As If” column.\nHourly gross salary per sex and full-time/part-time classification\n     | Nat All | Nat FT | Nat PT | Nat Male | Nat Male FT | Nat Male PT | Nat Female | Nat Female FT | Nat Female PT |\n     |---------|--------|--------|----------|-------------|-------------|------------|---------------|---------------|\nmean | 17.63 | 18.32 | 13.93 | 18.81 | 19.12 | 14.69 | 16.19 | 17.08 | 13.68 | median | 13.71 | 15.15 | 10.38 | 14.84 | 15.58 | 10.12 | 12.58 | 14.42 | 10.47 | M mean | 16.52 | 17.30 | 13.36 | 17.69 | 18.08 | 12.35 | 15.28 | 16.17 | 13.57 | M median | 13.79 | 14.71 | 10.30 | 14.75 | 15.16 | 9.16 | 12.80 | 14.10 | 10.55 |\nThe median values are quite close to the ONS values, but the mean values are always lower. This is expected, see the description of the modelling above.\nAnnual gross salary per sex and full-time/part-time classification\nOnly values > 0 are retained for these calculations.\n     | Nat All | Nat FT | Nat PT | Nat Male | Nat Male FT | Nat Male PT | Nat Female | Nat Female FT | Nat Female PT |\n     |---------|--------|--------|----------|-------------|-------------|------------|---------------|---------------|\nmean | 31,646 | 38,552 | 13,819 | 38,421 | 42,072 | 14,796 | 24,871 | 33,253 | 13,512 | median | 25,886 | 31,487 | 11,240 | 31,393 | 33,915 | 10,883 | 20,614 | 28,002 | 4,743 | M mean | 34,613 | 36,974 | 22,111 | 38,082 | 39,062 | 20,153 | 30,647 | 33,826 | 22,524 | M median | 29,249 | 31,625 | 17,947 | 32,441 | 33311 | 17,760 | 25,964 | 29,122 | 18,138 |\nThe average salary for part-time employees is correct when values equal to 0 are taken into account. This suggests that the total number of hours worked for part-time employees is correct, but the way they are distributed among individuals is not. It could be due to the TUS taking a snapshot of the situation during a particular week, rather than averaging their data over the year. It appears that the TUS matching also overestimates the average number of hours worked for female employees.\nRegional differences (hourly gross salary only)\n     | East  | East Midlands | London | North East | North West | South East | South West | West Midlands | Yorkshire and The Humber |\n     |-------|---------------|--------|------------|------------|------------|------------|---------------|--------------------------|\nmean | 16.74 | 15.87 | 23.78 | 15.69 | 16.36 | 17.88 | 16.36 | 16.34 | 15.76 | median | 13.28 | 12.65 | 18.30 | 12.40 | 12.90 | 14.33 | 12.74 | 12.92 | 12.46 | M mean | 16.68 | 15.39 | 19.38 | 5.27308 | 15.43 | 17.36 | 16.01 | 15.47 | 14.56 | M median | 13.91 | 13.00 | 16.42 | 12.64 | 12.78 | 14.95 | 13.66 | 12.89 | 12.72720 |\nThe pearson correlations for mean and median between the modelled and raw values are and 0.93 and 0.94.\nSOC (1-digit level, hourly gross salary only)\n     | 1     | 2     | 3     | 4     | 5     | 6     | 7     | 8     | 9     |\n     |-------|-------|-------|-------|-------|-------|-------|-------|-------|\nmean | 26.77 | 23.38 | 18.29 | 13.42 | 13.35 | 10.87 | 10.94 | 12.23 | 10.77 | median | 20.96 | 21.34 | 15.66 | 11.54 | 12.04 | 10.08 | 9.52 | 10.93 | 9.22 | M mean | 20.94 | 22.27 | 16.39 | 12.74 | 12.82 | 10.46 | 10.67 | 12.07 | 9.89 | M median | 17.06 | 20.78 | 14.68 | 11.58 | 11.74 | 9.85 | 9.73 | 10.96 | 9.14 |\n\nManagers, directors and senior officials\nProfessional occupations\nAssociate professional and technical occupations\nAdministrative and secretarial occupations\nSkilled trades occupations\nCaring, leisure and other service occupations\nSales and customer service occupations\nProcess, plant and machine operatives\nElementary occupations.\n\nThe pearson correlations for mean and median between the modelled and raw values are 0.97 and 0.97.\nAge\nThe reference for this table is: Table 6.5a Hourly pay - Gross 2020\n     | 16-17 | 18-21 | 22-29 | 30-39 | 40-49 | 50-59 | 60+   |\n     |-------|-------|-------|-------|-------|-------|-------|\nmean | 7.21 | 9.59 | 14.09 | 18.13 | 20.04 | 19.12 | 16.32 | median | 6.36 | 9.00 | 12.26 | 15.08 | 15.89 | 14.39 | 12.17 | M mean | 12.77 | 14.96 | 16.33 | 16.93 | 16.83 | 16.66 | 16.29 | M median | 10.93 | 12.71 | 13.88 | 14.02 | 13.96 | 13.85 | 13.65 |\nThe pearson correlations for mean and median between the modelled and raw values are 0.92 and 0.92.\nNote that age is not part of the inputs used to compute the estimations. It can therefore be considered as an independent control of the accuracy of the method and of the SPENSER/TUS/HSE matching."
  },
  {
    "objectID": "data_sources.html#utility-data",
    "href": "data_sources.html#utility-data",
    "title": "8  Data sources",
    "section": "8.1 Utility data",
    "text": "8.1 Utility data\nlookUp.csv\nThe look-up table links different geographies together. It is used internally by the model, but can also help the user define their own study area. MSOA11CD, MSOA11NM, LAD20CD, LAD20NM, ITL321CD, ITL321NM, ITL221CD, ITL221NM, ITL121CD, ITL121NM are all standard denominations fully compatible with ONS fields of the same name. They are based on ONS lookups. See ONS documentation for more details. CTY20NM and CCTY20NM are custom denominations for the counties of England (used to sort the county level population data) and the ceremonial counties of England respectively. Their spelling may vary in different data sources and the field CTY20NM is not compatible with the ONS field of the same name (which excludes all counties that are also unitary authorities). GoogleMob and OSM are different spellings for the counties of England used by Google and OSM for their data releases."
  },
  {
    "objectID": "data_sources.html#county-level-data",
    "href": "data_sources.html#county-level-data",
    "title": "8  Data sources",
    "section": "8.2 County level data",
    "text": "8.2 County level data\nContains 47 files, each representing the population in 2020 of one of the counties of England mentioned above, and named\ntus_hse_<county_name>.gz\nThis data is based on the 2011 UK census, the Time Use Survey 2014-15 and the Health Survey for England 2017. The SPENSER (Synthetic Population Estimation and Scenario Projection) microsimulation model (reference) distributes a synthetic population based on the census at MSOA scale and projects it to 2020 according to estimates from the Office for National Statistics (ONS). This information was enriched with some of the content of the other two datasets through propensity score matching (PSM) by Prof. Karyn Morrissey (Technical University of Denmark). The rest of the datasets can be added a posteriori from the identifiers provided.\nThe fields currently contained are:\n\nidp: a unique global individual identifier across all counties\nMSOA11CD: MSOA code where the individual lives\nhid: household identifier, includes communal establishments\npid: identifier linking to the 2011 Census\npid_tus: identifier linking to the Time Use Survey 2015\npid_hse: identifier linking to the Health Survey for England 2017\nsex: 0 female; 1 male\nage: in years\norigin: 1 White; 2 Black; 3 Asian; 4 Mixed; 5 Other\nnssec5: National Statistics Socio-economic classification:\n\n1: Higher managerial, administrative and professional occupations\n2: Intermediate occupations\n3: Small employers and own account workers\n4: Lower supervisory and technical occupations\n5: Semi-routine and routine occupations\n0: Never worked and long-term unemployed\n\nsoc2010: Previous version of the Standard Occupational Classification\nsic1d07: Standard Industrial Classification of Economic Activities 2007, 1st layer (number corresponding to the letter in alphabetical order)\nsic2d07: Standard Industrial Classification of Economic Activities 2007, 2nd layer\npwkstat: Employment status according to the TUS\nProportion of 24h spent doing different daily activities:\n\npunknown + pwork + pschool + pshop + pservices + pleisure + pescort + ptransport = pnothome\nphome + pworkhome = phometot\npnothome + phometot = 1\n\nIncomeX: hourly (X = “H”) and annual (X = “Y”) income for employees, see modelling methods for more details\ncvd: has a cardio-vascular disease (0 or 1)\ndiabetes: has diabetes (0 or 1)\nbloodpressure: has high blood pressure (0 or 1)\nBMIvg6: Body Mass Index:\n\nNot applicable\nUnderweight: less than 18.5\nNormal: 18.5 to less than 25\nOverweight: 25 to less than 30\nObese I: 30 to less than 35\nObese II: 35 to less than 40\nObese III: 40 or more\n\nlng: longitude of the MSOA11CD centroid\nlat: latitude of the MSOA11CD centroid\n\nSome other fields were kept from and for other specific projects but are not from official sources and should generally not be used."
  },
  {
    "objectID": "data_sources.html#national-data",
    "href": "data_sources.html#national-data",
    "title": "8  Data sources",
    "section": "8.3 National data",
    "text": "8.3 National data\nbusinessRegistry.csv\nContains a breakdown of all business units (i.e. a single workplace) in England at LSOA scale (smaller than MSOA), estimated by the project contributors from two nomis datasets: UK Business Counts - local units by industry and employment size band 2020 and Business Register and Employment Survey 2015. Each item contains the size of the unit and its main sic1d07 code in reference to standard Industrial Classification of Economic Activities 2007 (number corresponding to the letter in alphabetical order). It is used to compute commuting flows.\nThe R codes to compute this file are here.\nMSOAS_shp.tar.gz\nIs a simple shapefile taken from ONS boundaries.\nQUANT_RAMP.tar.gz\nSee: Milton R, Batty M, Dennett A, dedicated RAMP Spatial Interaction Model GitHub repository. It is used to compute the flows towards schools and retail.\ntimeAtHomeIncreaseCTY.csv\nThis file is a subset from Google COVID-19 Community Mobility Reports, cropped to England. It describes the daily reduction in mobility, averaged at county level, due to lockdown and other COVID-19 restrictions between the 15th of February 2020 and 15th of April 2022. Missing values have been replaced by the national average. These values can be used directly to reduce pnothome and increase phometot (and their sub-categories) to simulate more accurately the period.\nThe R codes to process these data are here."
  },
  {
    "objectID": "developer_guide.html#updating-the-docs",
    "href": "developer_guide.html#updating-the-docs",
    "title": "9  Developer guide",
    "section": "9.1 Updating the docs",
    "text": "9.1 Updating the docs\nThe site is built with Quarto. You can iterate on it locally: cd docs; quarto preview"
  },
  {
    "objectID": "developer_guide.html#code-hygiene",
    "href": "developer_guide.html#code-hygiene",
    "title": "9  Developer guide",
    "section": "9.2 Code hygiene",
    "text": "9.2 Code hygiene\nWe use automated tools to format the code.\ncargo fmt\n\n# Format Markdown docs\nprettier --write *.md\nprettier --write docs/*.qmd --parser markdown\nInstall prettier for Markdown."
  },
  {
    "objectID": "developer_guide.html#some-tips-for-working-with-rust",
    "href": "developer_guide.html#some-tips-for-working-with-rust",
    "title": "9  Developer guide",
    "section": "9.3 Some tips for working with Rust",
    "text": "9.3 Some tips for working with Rust\nThere are two equivalent ways to rebuild and then run the code. First:\ncargo run --release -- devon\nThe -- separates arguments to cargo, the Rust build tool, and arguments to the program itself. The second way:\ncargo build --release\n./target/release/aspics devon\nYou can build the code in two ways – debug and release. There’s a simple tradeoff – debug mode is fast to build, but slow to run. Release mode is slow to build, but fast to run. For the ASPICS codebase, since the input data is so large and the codebase so small, I’d recommend always using --release. If you want to use debug mode, just omit the flag.\nIf you’re working on the Rust code outside of an IDE like VSCode, then you can check if the code compiles much faster by doing cargo check."
  },
  {
    "objectID": "developer_guide.html#docker",
    "href": "developer_guide.html#docker",
    "title": "9  Developer guide",
    "section": "9.4 Docker",
    "text": "9.4 Docker\nWe provide a Dockerfile in case it’s helpful for running, but don’t recommend using it. If you want to, then assuming you have Docker setup:\ndocker build -t spc .\ndocker run --mount type=bind,source=\"$(pwd)\"/data,target=/spc/data -t spc /spc/target/release/spc config/bristol.txt\nThis will make the data directory in your directory available to the Docker image, where it’ll download the large input files and produce the final output."
  },
  {
    "objectID": "code_walkthrough.html#generally-useful-techniques",
    "href": "code_walkthrough.html#generally-useful-techniques",
    "title": "10  Code walkthrough",
    "section": "10.1 Generally useful techniques",
    "text": "10.1 Generally useful techniques\nThe code-base makes use of some techniques that may be generally applicable to other projects, independent of the language chosen.\n\n10.1.1 Split code into two stages\nAgent-based models and spatial interaction models require some kind of input. Often the effort to transform external data into this input can exceed that of the simulation component. Cleanly separating the two problems has some advantages:\n\niterate on the simulation faster, without processing raw data every run\nreuse the prepared input for future projects\nforce thinking about the data model needed by the simulation, and transform the external data into that form\n\nSPC is exactly this first stage, originally split from ASPICS when further uses of the same population data were identified.\n\n\n10.1.2 Explicit data schema\nDynamically typed languages like Python don’t force you to explicitly list the shape of input data. It’s common to read CSV files with pandas, filter and transform the data, and use that throughout the program. This can be quick to start prototyping, but is hard to maintain longer-term. Investing in the process of writing down types:\n\nmakes it easier for somebody new to understand your system – they can first focus on what you’re modeling, instead of how that’s built up from raw data sources\nclarifies what data actually matters to your system; you don’t carry forward unnecessary input\nmakes it impossible to express invalid states\n\nOne example is here – per person and activity, there’s a list of venues the person may visit, along with a probability of going there. If the list of venues and list of probabilities are stored as separate lists or columns, then their length may not match.\n\nreuse the prepared input for future projects\n\nThere’s a variety of techniques for expressing strongly typed data:\n\nprotocol buffers or flatbuffers\nJSON schemas\nPython data classes and optional type hints\nstatically typed languages like Rust\n\n\n\n10.1.3 Type-safe IDs\nSay your data model has many different objects, each with their own ID – people, households, venues, etc. You might store these in a list and use the index as an ID. This is fine, but nothing stops you from confusing IDs and accidentally passing in venue 5 to a function instead of household 5. In Rust, it’s easy to create “wrapper types” like this and let the compiler prevent these mistakes.\nThis technique is also useful when preparing external data. GTFS data describing public transit routes and timetables contains many string IDs – shapes, trips, stops, routes. As soon as you read the raw input, you can store the strings in more precise types that prevent mixing up a stop ID and route ID.\n\n\n10.1.4 Idempotent data preparation\nIf you’re iterating on your initialisation pipeline’s code, you probably don’t want to download a 2GB external file every single run. A common approach is to first test if a file exists and don’t download it again if so. In practice, you may also need to handle unzipping files, showing a progress bar while downloading, and printing clear error messages. This codebase has some common code for doing this in Rust. We intend to publish a separate library to more easily call in your own code.\n\n\n10.1.5 Logging with structure\nIt’s typical to print information as a complex pipeline runs, for the user to track progress and debug problems. But without any sort of organization, it’s hard to follow what steps take a long time or encounter problems. What if your logs could show the logical structure of your pipeline and help you understand where time is spent?\n\nThe screenshot above shows a summary printed at the end of a long pipeline run. It’s immediately obvious that the slowest step is creating commuting flows.\nThis codebase uses the tracing framework for logging, with a custom piece to draw the tree. (We’ll publish this as a separate library once it’s more polished.) The tracing framework is hard to understand, but the main conceptual leap over regular logging framworks is the concept of a span. When your code starts one logical step, you call a method to create a new span, and when it finishes, you close that span. Spans can be nested in any way – create_commuting_flows happens within the larger step of creating population.\n\n\n10.1.6 Determinism\nGiven the same inputs, your code should always produce identical output, no matter where it’s run or how many times. Otherwise, debugging problems becomes very tedious, and it’s more difficult to make conclusions from results. Of course, many projects have a stochastic element – but this should be controlled by a random number generator (RNG) seed, which is part of the input. You vary the seed and repeat the program, then reason about the distribution of results.\nAside from organizing your code to let a single RNG seed influence everything, another possible source of non-determinism is iteration order. In Rust, a HashMap could have different order every time it’s used, so we use a BTreeMap instead when this matters. In Python, dictionaries are ordered. Be sure to check for your language."
  },
  {
    "objectID": "code_walkthrough.html#protocol-buffers",
    "href": "code_walkthrough.html#protocol-buffers",
    "title": "10  Code walkthrough",
    "section": "10.2 Protocol buffers",
    "text": "10.2 Protocol buffers\nSPC uses protocol buffers for output. This has some advantages explained the “explicit data schema” section above, but the particular choice of protocol buffer has some limitations.\nFirst, proto3 doesn’t support required fields. This is done to allow schemas to evolve better over time, but this isn’t a feature SPC makes use of. There’s no need to have new code work with old data, or vice versa – if the schema is updated, downstream code should adapt accordingly and use the updated input files. The lack of required fields leads to imprecise code – a person’s health structure is always filled out, but in Rust, we wind up with Option<Health>. Differentiating 0 from missing data also becomes impossible – urn is optional, but in protobuf, we’re forced to map the missing case to 0 and document this.\nSecond, protocol buffers don’t easily support type-safe wrappers around numeric IDs, so downstream code has to be careful not to mix up household, venue, and person IDs.\nThird, protocol buffers support limited key types for maps. Enumerations can’t be used, so we use the numeric value for the activity enum.\nWe’ll evaluate flatbuffers and other alternative encodings.\nNote that in any case, SPC internally doesn’t use the auto-generated code until the very end of the pipeline. It’s always possible to be more precise with native Rust types, and convert to the less strict types last."
  },
  {
    "objectID": "code_walkthrough.html#an-example-of-the-power-of-static-type-checking",
    "href": "code_walkthrough.html#an-example-of-the-power-of-static-type-checking",
    "title": "10  Code walkthrough",
    "section": "10.3 An example of the power of static type checking",
    "text": "10.3 An example of the power of static type checking\nImagine we want to add a new activity type to represent people going to university and higher education. SPC already has activities for primary and secondary school, so we’ll probably want to follow those as a guide. In any language, we could search the codebase for relevant terms to get a sense of what to update. In languages like Python without an up-front compilation step, if we fail to update something or write blatantly incorrect code (such as making a typo in variable names or passing a list where a string was expected), we only find out when that code happens to run. In pipelines with many steps and large input files, it could be a while before we reach the problematic code.\nLet’s walk through the same exercise for SPC’s Rust code. We start by adding a new University case to the Activity enum. If we try to compile the code here (with cargo check or an IDE), we immediately get 4 errors.\n\nThree of the errors are in the QUANT module. The first is here. It’s immediately clear that for retail and primary/secondary school, we read in two files from QUANT representing venues where these activities take place and the probability of going to each venue. Even if we were unfamiliar with this codebase, the compiler has told us one thing we’ll need to figure out, and where to wire it up.\n\nThe other error is in the code that writes the protobuf output. Similarly, we need a way to represent university activities in the protobuf scheme.\nExtending an unfamiliar code-base backed by compiler errors is a very guided experience. If you wanted to add more demographic attributes to people or energy use information to households, you don’t need to guess all of the places in the code you’ll need to update. You can just add the field, then let the compiler tell you all places where those objects get created."
  },
  {
    "objectID": "performance.html",
    "href": "performance.html",
    "title": "11  Performance",
    "section": "",
    "text": "The following tables summarizes the resources SPC needs to run in different areas.\n\n\n\n\n\n\n\n\n\n\n\n\n\nstudy_area\nnum_msoas\nnum_households\nnum_people\npb_file_size\nruntime\ncommuting_runtime\nmemory_usage\n\n\n\n\nbedfordshire\n74\n271,487\n650,950\n94.33MiB\n13 seconds\n6 seconds\n293.76MiB\n\n\nberkshire\n107\n363,653\n878,045\n127.21MiB\n14 seconds\n7 seconds\n300.25MiB\n\n\nbristol\n55\n196,230\n456,532\n68.11MiB\n5 seconds\n2 seconds\n151.48MiB\n\n\nbuckinghamshire\n99\n324,843\n759,879\n109.30MiB\n10 seconds\n5 seconds\n297.42MiB\n\n\ncambridgeshire\n98\n346,532\n834,141\n120.30MiB\n13 seconds\n7 seconds\n300.23MiB\n\n\ncheshire\n139\n463,106\n1,040,634\n150.80MiB\n15 seconds\n6 seconds\n304.91MiB\n\n\ncornwall\n74\n246,873\n564,604\n83.30MiB\n7 seconds\n2 seconds\n277.31MiB\n\n\ncumbria\n64\n224,779\n485,035\n70.69MiB\n8 seconds\n3 seconds\n152.02MiB\n\n\nderbyshire\n131\n457,791\n1,024,952\n148.85MiB\n17 seconds\n9 seconds\n304.58MiB\n\n\ndevon\n156\n521,790\n1,178,315\n171.93MiB\n20 seconds\n10 seconds\n556.07MiB\n\n\ndorset\n95\n344,246\n751,334\n109.25MiB\n10 seconds\n5 seconds\n298.97MiB\n\n\ndurham\n117\n406,164\n904,785\n130.75MiB\n10 seconds\n4 seconds\n299.72MiB\n\n\neast_sussex\n102\n380,180\n830,761\n120.14MiB\n11 seconds\n5 seconds\n299.81MiB\n\n\neast_yorkshire_with_hull\n75\n261,267\n579,746\n85.31MiB\n7 seconds\n2 seconds\n278.80MiB\n\n\nessex\n211\n771,734\n1,798,893\n261.01MiB\n34 seconds\n23 seconds\n600.82MiB\n\n\ngloucestershire\n107\n392,120\n901,395\n129.84MiB\n13 seconds\n5 seconds\n302.14MiB\n\n\ngreater_london\n983\n3,135,814\n8,672,103\n1.23GiB\n11 minutes\n10 minutes\n4.32GiB\n\n\ngreater_manchester\n346\n871,651\n2,746,858\n389.21MiB\n2 minutes\n84 seconds\n1.08GiB\n\n\nhampshire\n225\n775,203\n1,803,991\n262.78MiB\n41 seconds\n29 seconds\n601.86MiB\n\n\nherefordshire\n23\n83,115\n191,282\n27.72MiB\n4 seconds\n1 second\n74.83MiB\n\n\nhertfordshire\n153\n492,783\n1,144,974\n165.59MiB\n19 seconds\n11 seconds\n555.75MiB\n\n\nisle_of_wight\n18\n64,602\n135,125\n20.48MiB\n3 seconds\n1 second\n70.05MiB\n\n\nkent\n220\n692,896\n1,808,206\n259.58MiB\n29 seconds\n18 seconds\n599.43MiB\n\n\nlancashire\n191\n603,524\n1,472,550\n210.80MiB\n22 seconds\n12 seconds\n592.20MiB\n\n\nleicestershire\n120\n417,621\n1,043,283\n147.79MiB\n17 seconds\n11 seconds\n302.29MiB\n\n\nlincolnshire\n134\n473,854\n1,064,174\n153.14MiB\n13 seconds\n6 seconds\n551.97MiB\n\n\nmerseyside\n184\n546,791\n1,401,012\n200.02MiB\n25 seconds\n17 seconds\n590.71MiB\n\n\nnorfolk\n110\n379,188\n891,006\n130.04MiB\n11 seconds\n4 seconds\n302.39MiB\n\n\nnorth_yorkshire\n138\n434,489\n1,069,514\n154.54MiB\n14 seconds\n6 seconds\n552.40MiB\n\n\nnorthamptonshire\n91\n293,580\n733,190\n106.21MiB\n10 seconds\n5 seconds\n297.24MiB\n\n\nnorthumberland\n40\n113,436\n316,618\n45.27MiB\n5 seconds\n1 second\n139.76MiB\n\n\nnorthwest_transpennine\n829\n2,378,868\n6,419,933\n928.03MiB\n6 minutes\n6 minutes\n2.32GiB\n\n\nnottinghamshire\n138\n413,097\n1,139,096\n163.50MiB\n19 seconds\n11 seconds\n553.29MiB\n\n\noxford_cambridge_arc\n353\n1,152,245\n2,823,838\n409.60MiB\n2 minutes\n78 seconds\n1.16GiB\n\n\noxfordshire\n86\n254,974\n669,237\n96.27MiB\n9 seconds\n4 seconds\n279.53MiB\n\n\nrutland\n5\n16,688\n39,475\n5.52MiB\n2 seconds\n1 second\n18.60MiB\n\n\nshropshire\n62\n153,284\n497,064\n70.66MiB\n6 seconds\n2 seconds\n150.19MiB\n\n\nsomerset\n124\n384,165\n944,394\n138.07MiB\n14 seconds\n6 seconds\n302.66MiB\n\n\nsouth_yorkshire\n172\n358,717\n1,373,401\n191.33MiB\n25 seconds\n18 seconds\n554.04MiB\n\n\nstaffordshire\n143\n439,176\n1,104,925\n157.81MiB\n14 seconds\n7 seconds\n552.70MiB\n\n\nsuffolk\n90\n326,760\n739,296\n106.71MiB\n9 seconds\n4 seconds\n296.93MiB\n\n\nsurrey\n151\n461,466\n1,136,090\n164.06MiB\n20 seconds\n13 seconds\n555.61MiB\n\n\ntyne_and_wear\n145\n414,128\n1,111,239\n157.15MiB\n12 seconds\n6 seconds\n551.14MiB\n\n\nwarwickshire\n108\n319,511\n933,391\n132.32MiB\n16 seconds\n10 seconds\n300.49MiB\n\n\nwest_midlands\n314\n523,264\n2,475,918\n348.50MiB\n38 seconds\n25 seconds\n1.05GiB\n\n\nwest_sussex\n100\n373,326\n838,440\n122.18MiB\n11 seconds\n4 seconds\n300.54MiB\n\n\nwest_yorkshire\n299\n960,426\n2,272,063\n331.50MiB\n42 seconds\n28 seconds\n1.08GiB\n\n\nwiltshire\n89\n305,679\n686,963\n100.69MiB\n8 seconds\n3 seconds\n295.39MiB\n\n\nworcestershire\n85\n254,383\n572,751\n83.91MiB\n8 seconds\n3 seconds\n277.79MiB\n\n\n\nNotes:\n\npb_file_size refers to the size of the uncompressed protobuf file in data/output/\nThe total runtime is usually dominated by matching workers to businesses, so commuting_runtime gives a breakdown\nMeasuring memory usage of Linux processes isn’t straightforward, so memory_usage should just be a guide\nThese measurements were all taken on one developer’s laptop, and they don’t represent multiple runs. This table just aims to give a general sense of how long running takes.\n\nThat machine has 16 cores, which matters for the parallelized commuting calculation.\n\nscripts/collect_stats.py produces the table above"
  }
]